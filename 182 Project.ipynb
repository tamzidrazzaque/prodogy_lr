{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall git+https://github.com/KellerJordan/Muon\n",
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yOaZmVEXPhW",
        "outputId": "9e05dd8b-34da-41a2-da2c-16563ab2c625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/KellerJordan/Muon\n",
            "  Cloning https://github.com/KellerJordan/Muon to /tmp/pip-req-build-7g63la4x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/KellerJordan/Muon /tmp/pip-req-build-7g63la4x\n",
            "  Resolved https://github.com/KellerJordan/Muon to commit f90a42b28e00b8d9d2d05865fe90d9f39abcbcbd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: muon-optimizer\n",
            "  Building wheel for muon-optimizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for muon-optimizer: filename=muon_optimizer-0.1.0-py3-none-any.whl size=7144 sha256=dbbe606f1c3387c090586d70350c0dfe39fb67eb82cb020a02c0b8e83ca0380b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tdf55xjm/wheels/6e/33/94/64d18603ba0f39064aab523d6edf493c388cfb7419bb5c9043\n",
            "Successfully built muon-optimizer\n",
            "Installing collected packages: muon-optimizer\n",
            "Successfully installed muon-optimizer-0.1.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg4o1-rtCsWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85849930-9a84-4a21-f1b6-b0629d45e421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from muon import SingleDeviceMuonWithAuxAdam  # the real Muon optimizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "\n",
        "def set_all_lrs(optimizer: Optimizer, lr: float) -> None:\n",
        "    \"\"\"Set the same learning rate for all param groups (Muon + Aux).\"\"\"\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg[\"lr\"] = float(lr)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LSState:\n",
        "    prev_lr: float\n",
        "    accepted_lr: float\n",
        "    last_num_evals: int = 0\n",
        "    # (optional) keep a running EMA of accepted lr for reporting\n",
        "    ema_lr: float = 0.0\n",
        "    ema_beta: float = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LineSearchMuon:\n",
        "    \"\"\"Prodigy-style batch line search on top of a Muon optimizer.\n",
        "\n",
        "    - Starts from previous accepted lr; first tries increasing (growth) while Armijo passes;\n",
        "      on first failure, accepts the largest passing lr. If nothing passes, it backtracks (shrink).\n",
        "    - Uses the *actual* data loss on the current batch for the sufficient-decrease check.\n",
        "    - Directional derivative is computed via g^T Δw where Δw = (w_new - w_old).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer: Optimizer,\n",
        "        init_lr: float = 1e-3,      # start in the good LR region from your sweep\n",
        "        c: float = 0.05,            # stricter Armijo (0.2 was far too loose)\n",
        "        growth: float = 1.2,        # slow growth so LR doesn’t explode\n",
        "        shrink: float = 0.3,        # stronger backtracking\n",
        "        lr_min: float = 1e-6,\n",
        "        lr_max: float = 0.01,       # based on your HP sweep winners (best region!)\n",
        "        max_grow: int = 4,\n",
        "        max_backtrack: int = 6,\n",
        "        eval_fraction: float = 1.0,\n",
        "        device: Optional[torch.device] = None,\n",
        "        use_weight_decay_in_objective: bool = False,\n",
        "    ):\n",
        "        self.opt = optimizer\n",
        "        self.state = LSState(prev_lr=float(init_lr),\n",
        "                             accepted_lr=float(init_lr),\n",
        "                             ema_lr=float(init_lr))\n",
        "        self.c = float(c)\n",
        "        self.growth = float(growth)\n",
        "        self.shrink = float(shrink)\n",
        "        self.lr_min = float(lr_min)\n",
        "        self.lr_max = float(lr_max)\n",
        "        self.max_grow = int(max_grow)\n",
        "        self.max_backtrack = int(max_backtrack)\n",
        "        self.eval_fraction = float(eval_fraction)\n",
        "        self.device = device\n",
        "        self.use_weight_decay_in_objective = bool(use_weight_decay_in_objective)\n",
        "\n",
        "        # Initialize optimizer lr\n",
        "        set_all_lrs(self.opt, init_lr)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _l2_weight_decay_term(self, model: nn.Module) -> float:\n",
        "        \"\"\"Optionally include L2 penalty in the objective for the line-search acceptance test.\n",
        "        We approximate a single global coefficient by averaging param-group weight_decay values.\n",
        "        \"\"\"\n",
        "        if not self.use_weight_decay_in_objective:\n",
        "            return 0.0\n",
        "        wds = [pg.get(\"weight_decay\", 0.0) for pg in self.opt.param_groups]\n",
        "        if not wds:\n",
        "            return 0.0\n",
        "        lam = float(sum(wds) / len(wds))\n",
        "        if lam == 0.0:\n",
        "            return 0.0\n",
        "        sq_sum = 0.0\n",
        "        for p in model.parameters():\n",
        "            if p.requires_grad:\n",
        "                sq_sum += float(torch.sum(p.detach() * p.detach()).item())\n",
        "        return 0.5 * lam * sq_sum\n",
        "\n",
        "    def _batch_slice(self, x: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Optionally subsample the batch to accelerate trial evaluations, without changing randomness.\"\"\"\n",
        "        if self.eval_fraction >= 1.0:\n",
        "            return x, y\n",
        "        n = x.shape[0]\n",
        "        m = max(1, int(n * self.eval_fraction))\n",
        "        return x[:m], y[:m]\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        criterion: nn.Module,\n",
        "    ) -> Tuple[float, float, int]:\n",
        "        \"\"\"Perform one optimizer step using line search.\n",
        "\n",
        "        Returns:\n",
        "            loss_value: float (full-batch loss before the update)\n",
        "            accepted_lr: float (lr used for the accepted step)\n",
        "            num_evals: int (number of trial forward evals performed for acceptance)\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "\n",
        "        # 1) Forward/backward at w0 to get L0 and grads\n",
        "        self.opt.zero_grad(set_to_none=True)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_value = float(loss.item())\n",
        "        loss.backward()\n",
        "\n",
        "        # -------- NAME-BASED SNAPSHOT (fixes shape mismatch) --------\n",
        "        params: Dict[str, torch.nn.Parameter] = {\n",
        "            name: p for name, p in model.named_parameters() if p.requires_grad\n",
        "        }\n",
        "        saved_grads: Dict[str, torch.Tensor] = {\n",
        "            name: (p.grad.detach().clone() if p.grad is not None else torch.zeros_like(p))\n",
        "            for name, p in params.items()\n",
        "        }\n",
        "\n",
        "        # Snapshot initial weights and optimizer state\n",
        "        w0_state = copy.deepcopy(model.state_dict())\n",
        "        opt0_state = copy.deepcopy(self.opt.state_dict())\n",
        "\n",
        "        # Helper to restore to w0 and reset grads\n",
        "        def restore_w0_and_grads():\n",
        "            model.load_state_dict(w0_state)\n",
        "            self.opt.load_state_dict(opt0_state)\n",
        "            for name, p in params.items():\n",
        "                p.grad = saved_grads[name].clone()\n",
        "\n",
        "        # Compute L0 (optionally + L2) on the eval slice without grad\n",
        "        with torch.no_grad():\n",
        "            xe, ye = self._batch_slice(inputs, targets)\n",
        "            L0_eval = float(criterion(model(xe), ye).item()) + self._l2_weight_decay_term(model)\n",
        "\n",
        "        num_evals = 0\n",
        "        best = None  # (lr, loss_eval, model_state, opt_state)\n",
        "\n",
        "        # ---------- Stage A: try to GROW from prev lr ----------\n",
        "        lr_try = float(min(max(self.state.prev_lr, self.lr_min), self.lr_max))\n",
        "        passes_any = False\n",
        "\n",
        "        for _ in range(self.max_grow):\n",
        "            restore_w0_and_grads()\n",
        "            set_all_lrs(self.opt, lr_try)\n",
        "            # Apply one update with the stored grads\n",
        "            self.opt.step()\n",
        "\n",
        "            # Evaluate acceptance condition\n",
        "            with torch.no_grad():\n",
        "                xe, ye = self._batch_slice(inputs, targets)\n",
        "                L_new = float(criterion(model(xe), ye).item()) + self._l2_weight_decay_term(model)\n",
        "            num_evals += 1\n",
        "\n",
        "            # Compute g^T Δw using saved grads and delta weights (name-based)\n",
        "            gtdw = 0.0\n",
        "            for name, p in params.items():\n",
        "                g = saved_grads[name]\n",
        "                dw = p.detach() - w0_state[name].to(p.device)  # correct tensor pairing\n",
        "                gtdw += float(torch.sum(g * dw).item())\n",
        "\n",
        "            # Armijo sufficient decrease: L(w + Δw) <= L(w) + c * g^T Δw\n",
        "            if L_new <= L0_eval + self.c * gtdw:\n",
        "                passes_any = True\n",
        "                best = (\n",
        "                    lr_try,\n",
        "                    L_new,\n",
        "                    copy.deepcopy(model.state_dict()),\n",
        "                    copy.deepcopy(self.opt.state_dict()),\n",
        "                )\n",
        "                # attempt to grow further\n",
        "                lr_try = min(lr_try * self.growth, self.lr_max)\n",
        "            else:\n",
        "                # first failure → stop growing\n",
        "                break\n",
        "\n",
        "        if not passes_any:\n",
        "            # ---------- Stage B: BACKTRACK from prev lr ----------\n",
        "            lr_try = max(self.state.prev_lr * self.shrink, self.lr_min)\n",
        "            for _ in range(self.max_backtrack):\n",
        "                restore_w0_and_grads()\n",
        "                set_all_lrs(self.opt, lr_try)\n",
        "                self.opt.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    xe, ye = self._batch_slice(inputs, targets)\n",
        "                    L_new = float(criterion(model(xe), ye).item()) + self._l2_weight_decay_term(model)\n",
        "                num_evals += 1\n",
        "\n",
        "                # Compute g^T Δw (name-based again)\n",
        "                gtdw = 0.0\n",
        "                for name, p in params.items():\n",
        "                    g = saved_grads[name]\n",
        "                    dw = p.detach() - w0_state[name].to(p.device)\n",
        "                    gtdw += float(torch.sum(g * dw).item())\n",
        "\n",
        "                if L_new <= L0_eval + self.c * gtdw:\n",
        "                    best = (\n",
        "                        lr_try,\n",
        "                        L_new,\n",
        "                        copy.deepcopy(model.state_dict()),\n",
        "                        copy.deepcopy(self.opt.state_dict()),\n",
        "                    )\n",
        "                    passes_any = True\n",
        "                    break\n",
        "\n",
        "                lr_try = max(lr_try * self.shrink, self.lr_min)\n",
        "\n",
        "        # ---------- Finalize ----------\n",
        "        if passes_any and best is not None:\n",
        "            accepted_lr, _, best_w, best_opt = best\n",
        "            model.load_state_dict(best_w)\n",
        "            self.opt.load_state_dict(best_opt)\n",
        "            self.state.accepted_lr = float(accepted_lr)\n",
        "            # modest optimism for next step\n",
        "            self.state.prev_lr = float(min(accepted_lr * self.growth, self.lr_max))\n",
        "        else:\n",
        "            # No step satisfied Armijo; we accept NO UPDATE and reduce prev lr.\n",
        "            restore_w0_and_grads()\n",
        "            self.state.prev_lr = float(max(self.state.prev_lr * self.shrink, self.lr_min))\n",
        "            self.state.accepted_lr = 0.0  # signal skipped\n",
        "\n",
        "        # Book-keeping\n",
        "        self.state.ema_lr = (\n",
        "            self.state.ema_lr * self.state.ema_beta +\n",
        "            (1 - self.state.ema_beta) * max(self.state.accepted_lr, 0.0)\n",
        "        )\n",
        "        self.state.last_num_evals = num_evals\n",
        "\n",
        "        # Return the full-batch *pre-update* loss (for logging), accepted lr, and eval count\n",
        "        return loss_value, float(self.state.accepted_lr), num_evals"
      ],
      "metadata": {
        "id": "KLic142ZC5qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_ls(\n",
        "    model: nn.Module,\n",
        "    ls_ctl: LineSearchMuon,\n",
        "    trainloader: Iterable[Tuple[torch.Tensor, torch.Tensor]],\n",
        "    device: torch.device,\n",
        "    criterion: nn.Module,\n",
        "    log_every: int = 100,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"Train for exactly one epoch using the line-search controller.\n",
        "\n",
        "    Returns:\n",
        "        avg_loss, avg_acc, avg_accepted_lr\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    lr_accum = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(trainloader, 1):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        # one LS-controlled step\n",
        "        loss_val, accepted_lr, _ = ls_ctl.step(model, inputs, targets, criterion)\n",
        "\n",
        "        # compute batch accuracy from the forward we already did (do another forward cheaply)\n",
        "        with torch.no_grad():\n",
        "            logits = model(inputs)\n",
        "            preds = logits.argmax(1)\n",
        "            running_correct += int((preds == targets).sum().item())\n",
        "            running_total += int(targets.size(0))\n",
        "\n",
        "        running_loss += loss_val * targets.size(0)\n",
        "        lr_accum += max(accepted_lr, 0.0)\n",
        "        steps += 1\n",
        "\n",
        "        if (i % log_every) == 0:\n",
        "            avg_tr_loss = running_loss / max(1, running_total)\n",
        "            avg_tr_acc = 100.0 * running_correct / max(1, running_total)\n",
        "            avg_lr = lr_accum / max(1, steps)\n",
        "            print(f\"[LS] step {i:04d}/{len(trainloader)} | loss {avg_tr_loss:.4f} | acc {avg_tr_acc:5.2f}% | \"\n",
        "                  f\"lr~{avg_lr:.3g} | evals/step~{ls_ctl.state.last_num_evals}\")\n",
        "\n",
        "    avg_loss = running_loss / max(1, running_total)\n",
        "    avg_acc = running_correct / max(1, running_total)\n",
        "    avg_lr = lr_accum / max(1, steps) if steps > 0 else 0.0\n",
        "    return float(avg_loss), float(avg_acc), float(avg_lr)"
      ],
      "metadata": {
        "id": "E3_13saXC_SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
        "CIFAR10_STD  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "test_tfms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD),\n",
        "])\n",
        "\n",
        "train_ds = torchvision.datasets.CIFAR10(\"./data\", train=True,\n",
        "                                        download=True, transform=train_tfms)\n",
        "test_ds  = torchvision.datasets.CIFAR10(\"./data\", train=False,\n",
        "                                        download=True, transform=test_tfms)\n",
        "\n",
        "batch_size = 128\n",
        "num_workers = 2 if device.type == \"cuda\" else 0\n",
        "\n",
        "trainloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                         num_workers=num_workers, pin_memory=True)\n",
        "testloader  = DataLoader(test_ds,  batch_size=256, shuffle=False,\n",
        "                         num_workers=num_workers, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "bO6k4herXjwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet18_cifar(num_classes=10):\n",
        "    m = models.resnet18(weights=None)\n",
        "    # Modify for CIFAR (optional)\n",
        "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    return m.to(device)\n",
        "\n",
        "model = resnet18_cifar()\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "8bSHHtcXXkz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "muon_params, aux_params = [], []\n",
        "\n",
        "for name, p in model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    # Muon handles 2D matrices\n",
        "    (muon_params if p.ndim == 2 else aux_params).append(p)\n",
        "\n",
        "param_groups = [\n",
        "    dict(params=muon_params, use_muon=True,\n",
        "         lr=2e-2, momentum=0.95, weight_decay=0.0005),\n",
        "    dict(params=aux_params,  use_muon=False,\n",
        "         lr=1e-3, weight_decay=0.0005),\n",
        "]\n",
        "\n",
        "optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n"
      ],
      "metadata": {
        "id": "Sq_GyqHIXmoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_ctl = LineSearchMuon(optimizer)\n"
      ],
      "metadata": {
        "id": "mRaJihEyXtUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "epochs = 10\n",
        "best = 0.\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    tr_loss, tr_acc, tr_lr = train_one_epoch_ls(model, ls_ctl, trainloader, device, criterion)\n",
        "    val_loss, val_acc = evaluate(model)\n",
        "\n",
        "    print(f\"[Epoch {epoch}] loss={val_loss:.4f} acc={val_acc*100:.2f}% lr~{tr_lr:.4f}\")\n",
        "\n",
        "    if val_acc > best:\n",
        "        best = val_acc\n",
        "\n",
        "print(\"Best accuracy:\", best*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NikZz-4QXu8G",
        "outputId": "07528873-c534-4f01-9405-283b389681b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LS] step 0100/391 | loss 1.9904 | acc 33.06% | lr~0.00626 | evals/step~2\n",
            "[LS] step 0200/391 | loss 1.8317 | acc 37.98% | lr~0.0062 | evals/step~4\n",
            "[LS] step 0300/391 | loss 1.7309 | acc 41.83% | lr~0.0065 | evals/step~2\n",
            "[Epoch 1] loss=1.3067 acc=51.49% lr~0.0067\n",
            "[LS] step 0100/391 | loss 1.2645 | acc 62.80% | lr~0.00828 | evals/step~4\n",
            "[LS] step 0200/391 | loss 1.2115 | acc 65.23% | lr~0.00881 | evals/step~4\n",
            "[LS] step 0300/391 | loss 1.1565 | acc 67.42% | lr~0.00889 | evals/step~4\n",
            "[Epoch 2] loss=0.9764 acc=66.37% lr~0.0090\n",
            "[LS] step 0100/391 | loss 0.9013 | acc 78.55% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.8767 | acc 79.34% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.8519 | acc 80.24% | lr~0.01 | evals/step~4\n",
            "[Epoch 3] loss=0.7321 acc=75.40% lr~0.0100\n",
            "[LS] step 0100/391 | loss 0.6852 | acc 85.80% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.6742 | acc 86.19% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.6650 | acc 86.55% | lr~0.01 | evals/step~4\n",
            "[Epoch 4] loss=0.6154 acc=79.49% lr~0.0100\n",
            "[LS] step 0100/391 | loss 0.5806 | acc 89.43% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.5674 | acc 89.78% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.5605 | acc 90.03% | lr~0.01 | evals/step~4\n",
            "[Epoch 5] loss=0.5465 acc=81.85% lr~0.0100\n",
            "[LS] step 0100/391 | loss 0.4946 | acc 91.80% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.4908 | acc 91.96% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.4902 | acc 91.95% | lr~0.01 | evals/step~4\n",
            "[Epoch 6] loss=0.4930 acc=83.61% lr~0.0100\n",
            "[LS] step 0100/391 | loss 0.4322 | acc 93.72% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.4282 | acc 93.64% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.4320 | acc 93.62% | lr~0.01 | evals/step~4\n",
            "[Epoch 7] loss=0.4689 acc=84.35% lr~0.0100\n",
            "[LS] step 0100/391 | loss 0.3903 | acc 94.70% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.3938 | acc 94.71% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.3947 | acc 94.61% | lr~0.01 | evals/step~4\n",
            "[Epoch 8] loss=0.4657 acc=84.53% lr~0.0100\n",
            "[LS] step 0100/391 | loss 0.3524 | acc 95.59% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.3560 | acc 95.67% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.3544 | acc 95.63% | lr~0.01 | evals/step~4\n",
            "[Epoch 9] loss=0.4123 acc=86.67% lr~0.0100\n",
            "[LS] step 0100/391 | loss 0.3284 | acc 96.35% | lr~0.01 | evals/step~4\n",
            "[LS] step 0200/391 | loss 0.3226 | acc 96.34% | lr~0.01 | evals/step~4\n",
            "[LS] step 0300/391 | loss 0.3222 | acc 96.32% | lr~0.01 | evals/step~4\n",
            "[Epoch 10] loss=0.4170 acc=86.85% lr~0.0100\n",
            "Best accuracy: 86.85000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aAxHOWwzX1gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# BASELINE: STANDARD MUON RUN\n",
        "# ===========================\n",
        "\n",
        "# CIFAR-10 friendly ResNet-18\n",
        "# (first conv = 3x3, stride 1; no maxpool)\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "def resnet18_cifar(num_classes=10):\n",
        "    # Load ResNet-18 without pretrained weights\n",
        "    model = models.resnet18(weights=None)\n",
        "\n",
        "    # Replace the first conv layer for CIFAR-10\n",
        "    model.conv1 = nn.Conv2d(\n",
        "        3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
        "    )\n",
        "\n",
        "    # Remove the maxpool layer\n",
        "    model.maxpool = nn.Identity()\n",
        "\n",
        "    # Replace the classifier\n",
        "    model.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "baseline_model = resnet18_cifar().to(device)\n",
        "\n",
        "# Separate Muon params (2D) and Aux (1D)\n",
        "muon_params, aux_params = [], []\n",
        "for name, p in baseline_model.named_parameters():\n",
        "    if not p.requires_grad:\n",
        "        continue\n",
        "    (muon_params if p.ndim == 2 else aux_params).append(p)\n",
        "\n",
        "# Baseline: fixed Muon learning rate (tuned default)\n",
        "BASELINE_LR = 2e-3   # <-- you can try 1e-3, 2e-3, 5e-3\n",
        "\n",
        "param_groups_baseline = [\n",
        "    dict(params=muon_params, use_muon=True,  lr=BASELINE_LR, momentum=0.95, weight_decay=0.0005),\n",
        "    dict(params=aux_params,  use_muon=False, lr=1e-3,        weight_decay=0.0005),\n",
        "]\n",
        "\n",
        "baseline_opt = SingleDeviceMuonWithAuxAdam(param_groups_baseline)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_baseline(model):\n",
        "    model.eval()\n",
        "    correct, total, loss_sum = 0, 0, 0\n",
        "    for x, y in testloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += y.size(0)\n",
        "    return loss_sum / total, correct / total\n",
        "\n",
        "\n",
        "def train_one_epoch_baseline(model, optimizer):\n",
        "    model.train()\n",
        "    running_loss, running_correct, running_total = 0, 0, 0\n",
        "    for x, y in trainloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * y.size(0)\n",
        "        running_correct += (logits.argmax(1) == y).sum().item()\n",
        "        running_total += y.size(0)\n",
        "\n",
        "    return (\n",
        "        running_loss / running_total,\n",
        "        running_correct / running_total,\n",
        "    )\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Run baseline for comparison\n",
        "# ===========================\n",
        "\n",
        "epochs_baseline = 10\n",
        "best_baseline = 0.0\n",
        "\n",
        "print(\"\\n=========== BASELINE MUON RUN ===========\")\n",
        "\n",
        "for epoch in range(1, epochs_baseline + 1):\n",
        "    tr_loss_base, tr_acc_base = train_one_epoch_baseline(baseline_model, baseline_opt)\n",
        "    val_loss_base, val_acc_base = evaluate_baseline(baseline_model)\n",
        "\n",
        "    print(f\"[Epoch {epoch}] loss={val_loss_base:.4f} acc={val_acc_base*100:.2f}%  (train acc {tr_acc_base*100:.2f}%)\")\n",
        "\n",
        "    if val_acc_base > best_baseline:\n",
        "        best_baseline = val_acc_base\n",
        "\n",
        "print(\"\\nBaseline best accuracy:\", best_baseline * 100, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUo5dbsoy65p",
        "outputId": "03c2d7b0-9104-4197-cb46-5b45d7451ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=========== BASELINE MUON RUN ===========\n",
            "[Epoch 1] loss=1.1520 acc=61.46%  (train acc 51.26%)\n",
            "[Epoch 2] loss=0.8173 acc=71.21%  (train acc 69.44%)\n",
            "[Epoch 3] loss=0.7776 acc=73.67%  (train acc 76.14%)\n",
            "[Epoch 4] loss=0.6495 acc=78.40%  (train acc 80.00%)\n",
            "[Epoch 5] loss=0.5129 acc=82.04%  (train acc 82.62%)\n",
            "[Epoch 6] loss=0.4818 acc=84.06%  (train acc 84.63%)\n",
            "[Epoch 7] loss=0.4283 acc=85.76%  (train acc 86.18%)\n",
            "[Epoch 8] loss=0.4295 acc=85.58%  (train acc 87.42%)\n",
            "[Epoch 9] loss=0.4870 acc=84.08%  (train acc 88.66%)\n",
            "[Epoch 10] loss=0.3959 acc=86.79%  (train acc 89.31%)\n",
            "\n",
            "Baseline best accuracy: 86.79 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot LS-Muon vs tuned Muon accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = list(range(1, 11))\n",
        "ls_acc = [51.49, 66.37, 75.40, 79.49, 81.85, 83.61, 84.35, 84.53, 86.67, 86.85]\n",
        "baseline_acc = [61.46, 71.21, 73.67, 78.40, 82.04, 84.06, 85.76, 85.58, 84.08, 86.79]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, ls_acc, label=\"LS-Muon\", marker=\"o\")\n",
        "plt.plot(epochs, baseline_acc, label=\"Tuned Muon\", marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Validation Accuracy (%)\")\n",
        "plt.title(\"LS-Muon vs Tuned Muon\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.savefig(\"lsmuon_vs_baseline_acc.png\", dpi=200)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "CdEAVg0KtONp",
        "outputId": "198c3ffd-53bd-44cf-daf2-34d42976bd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot LS-Muon learning rate schedule\n",
        "ls_lr = [0.0063, 0.0083, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(epochs, ls_lr, marker=\"o\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Armijo-Muon Adaptive Learning Rate\")\n",
        "plt.grid()\n",
        "plt.savefig(\"lsmuon_lr_curve.png\", dpi=200)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "aA0TrfVQtSrI",
        "outputId": "e210a231-e15d-4b71-b931-cae9926bed7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to use it with Jason/Johan current code:\n",
        "# from ls_muon_line_search import LineSearchMuon, train_one_epoch_ls, set_all_lrs\n",
        "from muon import SingleDeviceMuonWithAuxAdam\n",
        "optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
        "\n",
        "# initialize LS controller (tune hyperparams if you like)\n",
        "ls_ctl = LineSearchMuon(\n",
        "    optimizer,\n",
        "    init_lr=2e-2,   # start near what worked for Muon in your baseline\n",
        "    c=0.2,          # Armijo coefficient; 0.1–0.25 are common\n",
        "    growth=1.4,     # how aggressively to try larger steps\n",
        "    shrink=0.5,     # backtracking factor on failure\n",
        "    lr_min=5e-6,    # floor\n",
        "    lr_max=1.0,     # ceiling\n",
        "    max_grow=3,     # at most this many growth trials per batch\n",
        "    max_backtrack=6,# and backtracking trials\n",
        "    eval_fraction=1.0,  # use full batch for acceptance test; consider 0.5 if you want it cheaper\n",
        ")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # OLD:\n",
        "    # train_one_epoch(epoch)\n",
        "\n",
        "    # NEW:\n",
        "    tr_loss, tr_acc, tr_lr = train_one_epoch_ls(model, ls_ctl, trainloader, device, criterion)\n",
        "\n",
        "    val_loss, val_acc = evaluate()\n",
        "    print(f\"[Eval] epoch {epoch}: loss={val_loss:.4f} acc={val_acc*100:.2f}% (train lr~{tr_lr:.3g})\")\n",
        "    if val_acc > best:\n",
        "        best = val_acc\n",
        "        torch.save(model.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "zTsztRDbDIwg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "017c9db3-f3ec-49f1-faad-2136be6e1fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'param_groups' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-280357671.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from ls_muon_line_search import LineSearchMuon, train_one_epoch_ls, set_all_lrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmuon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleDeviceMuonWithAuxAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleDeviceMuonWithAuxAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# initialize LS controller (tune hyperparams if you like)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'param_groups' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sweeping Code"
      ],
      "metadata": {
        "id": "CN3q4JzYSTyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "from ls_muon_line_search import LineSearchMuon, train_one_epoch_ls, set_all_lrs\n",
        "from torch import nn\n"
      ],
      "metadata": {
        "id": "AuosucDDDOZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                         (0.2470, 0.2435, 0.2616)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=128,\n",
        "                         shuffle=True, num_workers=2)\n",
        "testloader  = DataLoader(testset, batch_size=256,\n",
        "                         shuffle=False, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "id": "uNn_6g1lSEr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def build_model():\n",
        "    model = resnet18(num_classes=10)\n",
        "    return model.to(device)\n"
      ],
      "metadata": {
        "id": "xS-Vhwh2SGFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, loader=testloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    loss_sum = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss_sum += loss.item() * y.size(0)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "    return loss_sum / total, correct / total\n"
      ],
      "metadata": {
        "id": "eBGrpK9hSHQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(init_lr, c, growth, shrink):\n",
        "    print(f\"=== Running experiment: init_lr={init_lr}, c={c}, growth={growth}, shrink={shrink}\")\n",
        "\n",
        "    model = build_model()\n",
        "    param_groups = [{\"params\": model.parameters()}]\n",
        "\n",
        "    optimizer = SingleDeviceMuonWithAuxAdam(param_groups)\n",
        "\n",
        "    ls_ctl = LineSearchMuon(\n",
        "        optimizer,\n",
        "        init_lr=init_lr,\n",
        "        c=c,\n",
        "        growth=growth,\n",
        "        shrink=shrink,\n",
        "        lr_min=5e-6,\n",
        "        lr_max=1.0,\n",
        "        max_grow=3,\n",
        "        max_backtrack=6,\n",
        "        eval_fraction=1.0,\n",
        "    )\n",
        "\n",
        "    best = 0.0\n",
        "    EPOCHS = 20\n",
        "\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        tr_loss, tr_acc, tr_lr = train_one_epoch_ls(\n",
        "            model, ls_ctl, trainloader, device, criterion\n",
        "        )\n",
        "        val_loss, val_acc = evaluate(model)\n",
        "\n",
        "        print(f\"[Epoch {epoch}] val_acc={val_acc*100:.2f}%  train_lr~{tr_lr:.4f}\")\n",
        "\n",
        "        best = max(best, val_acc)\n",
        "\n",
        "    print(f\"=== DONE | Best val_acc={best*100:.2f}% ===\")\n",
        "    return best\n"
      ],
      "metadata": {
        "id": "Y3tyJC1sSIjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_space = [\n",
        "    (1e-2, 0.1, 1.3, 0.5),\n",
        "    (2e-2, 0.2, 1.4, 0.5),\n",
        "    (3e-2, 0.2, 1.5, 0.6),\n",
        "    (5e-3, 0.25, 1.4, 0.5),\n",
        "]\n"
      ],
      "metadata": {
        "id": "U5r_3F90SLef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for (init_lr, c, growth, shrink) in sweep_space:\n",
        "    best_val = run_experiment(init_lr, c, growth, shrink)\n",
        "    results.append((init_lr, c, growth, shrink, best_val))\n"
      ],
      "metadata": {
        "id": "tlfEVqUBSMh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n===== SWEEP SUMMARY =====\")\n",
        "for (init_lr, c, growth, shrink, acc) in results:\n",
        "    print(f\"init_lr={init_lr}, c={c}, growth={growth}, shrink={shrink}  →  val_acc={acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "2sNrKZP5SNr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "690dfc36"
      },
      "source": [
        "# Task\n",
        "Load the CIFAR-10 dataset and define a 5-layer Multi-Layer Perceptron (MLP) model. Prepare `DataLoader`s for both training and validation with a batch size of 256. Initialize a `torch.optim.Adam` optimizer for the MLP, and then initialize the `LineSearchMuon` controller with this optimizer and the following hyperparameters: `init_lr=2e-2`, `c=0.2`, `growth=1.4`, `shrink=0.5`, `lr_min=5e-6`, `lr_max=1.0`, `max_grow=3`, `max_backtrack=6`, and `eval_fraction=1.0`. Define an `evaluate` function to calculate loss and accuracy on the CIFAR-10 test dataset. Finally, execute a training loop for 200 epochs using the `train_one_epoch_ls` function, incorporating the CIFAR-10 data, MLP model, line search controller, and a suitable criterion. Periodically log training progress and validate the model. Summarize the results and observations from training, highlighting any insights into the optimizer's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7048452e"
      },
      "source": [
        "## Load CIFAR-10 Dataset and Define MLP\n",
        "\n",
        "### Subtask:\n",
        "Load the CIFAR-10 training and test datasets. Define a 5-layer Multi-Layer Perceptron (MLP) model suitable for CIFAR-10 images. Prepare `DataLoader`s with a batch size of 256 for both training and validation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0496358e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires importing several libraries and setting the device for PyTorch operations. This is the first step in the instructions provided.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85c6e32d",
        "outputId": "1278de5c-943f-438e-e804-262643c04049"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5307541"
      },
      "source": [
        "**Reasoning**:\n",
        "Following the instructions, I will define the transformations for CIFAR-10, load the training and test datasets, and create DataLoaders for both, using a batch size of 256. This directly addresses steps 2, 3, and 4 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e376c42",
        "outputId": "b3805dbd-ddc0-4683-89d9-883bea4bb07f"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 training and test datasets\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
        "\n",
        "print(\"CIFAR-10 datasets loaded and DataLoaders prepared.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 66.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 datasets loaded and DataLoaders prepared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40854f81"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to define the 5-layer MLP model, instantiate it, and move it to the configured device, as per instructions 5 and 6 of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dcf14ff",
        "outputId": "d131b3bf-25a1-46b4-abdf-1e1b24ddc549"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc5 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.relu3(self.fc3(x))\n",
        "        x = self.relu4(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the MLP model and move it to the device\n",
        "model = MLP().to(device)\n",
        "print(\"5-layer MLP model defined and moved to device.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-layer MLP model defined and moved to device.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b54233e"
      },
      "source": [
        "## Initialize Optimizer and LineSearchMuon\n",
        "\n",
        "### Subtask:\n",
        "Initialize a standard `torch.optim.Adam` optimizer for the 5-layer MLP model. Then, initialize your `LineSearchMuon` controller with this optimizer and the specified hyperparameters, ensuring it aligns with the project proposal's methodology.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed116c7b"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires initializing a torch.optim.Adam optimizer, defining the criterion, and then initializing the LineSearchMuon controller with specific hyperparameters. This code block addresses all these requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "f079afc5",
        "outputId": "56d9538e-a1e2-4adf-f493-eab4a410d287"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) # Nominal LR, will be overridden by LS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize LineSearchMuon controller\n",
        "ls_ctl = LineSearchMuon(\n",
        "    optimizer,\n",
        "    init_lr=2e-2,\n",
        "    c=0.2,\n",
        "    growth=1.4,\n",
        "    shrink=0.5,\n",
        "    lr_min=5e-6,\n",
        "    lr_max=1.0,\n",
        "    max_grow=3,\n",
        "    max_backtrack=6,\n",
        "    eval_fraction=1.0,\n",
        ")\n",
        "\n",
        "print(\"Optimizer, criterion, and LineSearchMuon controller initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LineSearchMuon' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2024006501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize LineSearchMuon controller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m ls_ctl = LineSearchMuon(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minit_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LineSearchMuon' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f8a3430"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `NameError` because `LineSearchMuon` was not defined in the current scope. Although `LineSearchMuon` is defined in an earlier notebook cell, the provided 'How to use it' example (`zTsztRDbDIwg`) suggests it should be imported from a module named `ls_muon_line_search`. To resolve the `NameError`, I will add the necessary import statement for `LineSearchMuon` to the cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "1721f8b3",
        "outputId": "caa1bfbb-552b-451e-fea2-e383aa0b8deb"
      },
      "source": [
        "from ls_muon_line_search import LineSearchMuon\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) # Nominal LR, will be overridden by LS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize LineSearchMuon controller\n",
        "ls_ctl = LineSearchMuon(\n",
        "    optimizer,\n",
        "    init_lr=2e-2,\n",
        "    c=0.2,\n",
        "    growth=1.4,\n",
        "    shrink=0.5,\n",
        "    lr_min=5e-6,\n",
        "    lr_max=1.0,\n",
        "    max_grow=3,\n",
        "    max_backtrack=6,\n",
        "    eval_fraction=1.0,\n",
        ")\n",
        "\n",
        "print(\"Optimizer, criterion, and LineSearchMuon controller initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ls_muon_line_search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3338568412.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mls_muon_line_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLineSearchMuon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Nominal LR, will be overridden by LS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ls_muon_line_search'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caff9279"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `ModuleNotFoundError` because `LineSearchMuon` was attempted to be imported from a non-existent module `ls_muon_line_search`. The `LineSearchMuon` class is already defined directly in an earlier notebook cell, making it available in the global scope. Therefore, the import statement is unnecessary and should be removed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "b4feb518",
        "outputId": "0daf0802-67e0-4dc0-f80a-e7517d0a87c2"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) # Nominal LR, will be overridden by LS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize LineSearchMuon controller\n",
        "ls_ctl = LineSearchMuon(\n",
        "    optimizer,\n",
        "    init_lr=2e-2,\n",
        "    c=0.2,\n",
        "    growth=1.4,\n",
        "    shrink=0.5,\n",
        "    lr_min=5e-6,\n",
        "    lr_max=1.0,\n",
        "    max_grow=3,\n",
        "    max_backtrack=6,\n",
        "    eval_fraction=1.0,\n",
        ")\n",
        "\n",
        "print(\"Optimizer, criterion, and LineSearchMuon controller initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LineSearchMuon' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2024006501.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize LineSearchMuon controller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m ls_ctl = LineSearchMuon(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minit_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LineSearchMuon' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990edc36"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `NameError` because the `LineSearchMuon` class, along with its dependencies (`LSState` and `set_all_lrs`), was not defined in the current execution scope, despite being present in earlier notebook cells. To ensure the code runs correctly, I must include the full definitions of these components within the current code block before attempting to instantiate `LineSearchMuon`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9910a36a",
        "outputId": "be87af6c-f2e9-43db-96de-f151de4376d6"
      },
      "source": [
        "import copy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Optimizer\n",
        "\n",
        "def set_all_lrs(optimizer: Optimizer, lr: float) -> None:\n",
        "    \"\"\"Set the same learning rate for all param groups (Muon + Aux).\"\"\"\n",
        "    for pg in optimizer.param_groups:\n",
        "        pg[\"lr\"] = float(lr)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LSState:\n",
        "    prev_lr: float\n",
        "    accepted_lr: float\n",
        "    last_num_evals: int = 0\n",
        "    # (optional) keep a running EMA of accepted lr for reporting\n",
        "    ema_lr: float = 0.0\n",
        "    ema_beta: float = 0.9\n",
        "\n",
        "class LineSearchMuon:\n",
        "    \"\"\"Prodigy-style batch line search on top of a Muon optimizer.\n",
        "\n",
        "    - Starts from previous accepted lr; first tries increasing (growth) while Armijo passes;\n",
        "      on first failure, accepts the largest passing lr. If nothing passes, it backtracks (shrink).\n",
        "    - Uses the *actual* data loss on the current batch for the sufficient-decrease check.\n",
        "    - Directional derivative is computed via g^T Δw where Δw = (w_new - w_old).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer: Optimizer,\n",
        "        init_lr: float,\n",
        "        c: float = 0.2,            # Armijo fraction (0 < c < 1); larger => easier to accept\n",
        "        growth: float = 1.4,       # multiplicative factor to try larger steps\n",
        "        shrink: float = 0.5,       # multiplicative factor to backtrack\n",
        "        lr_min: float = 1e-6,\n",
        "        lr_max: float = 1.0,\n",
        "        max_grow: int = 4,\n",
        "        max_backtrack: int = 8,\n",
        "        eval_fraction: float = 1.0,  # use a fraction of the batch for trial-eval to save time\n",
        "        device: Optional[torch.device] = None,\n",
        "        use_weight_decay_in_objective: bool = False,\n",
        "    ):\n",
        "        self.opt = optimizer\n",
        "        self.state = LSState(prev_lr=float(init_lr), accepted_lr=float(init_lr), ema_lr=float(init_lr))\n",
        "        self.c = float(c)\n",
        "        self.growth = float(growth)\n",
        "        self.shrink = float(shrink)\n",
        "        self.lr_min = float(lr_min)\n",
        "        self.lr_max = float(lr_max)\n",
        "        self.max_grow = int(max_grow)\n",
        "        self.max_backtrack = int(max_backtrack)\n",
        "        self.eval_fraction = float(eval_fraction)\n",
        "        self.device = device\n",
        "        self.use_weight_decay_in_objective = bool(use_weight_decay_in_objective)\n",
        "\n",
        "        # Initialize optimizer lr\n",
        "        set_all_lrs(self.opt, init_lr)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _l2_weight_decay_term(self, model: nn.Module) -> float:\n",
        "        \"\"\"Optionally include L2 penalty in the objective for the line-search acceptance test.\n",
        "        We approximate a single global coefficient by averaging param-group weight_decay values.\n",
        "        \"\"\"\n",
        "        if not self.use_weight_decay_in_objective:\n",
        "            return 0.0\n",
        "        wds = [pg.get(\"weight_decay\", 0.0) for pg in self.opt.param_groups]\n",
        "        if not wds:\n",
        "            return 0.0\n",
        "        lam = float(sum(wds) / len(wds))\n",
        "        if lam == 0.0:\n",
        "            return 0.0\n",
        "        sq_sum = 0.0\n",
        "        for p in model.parameters():\n",
        "            if p.requires_grad:\n",
        "                sq_sum += float(torch.sum(p.detach() * p.detach()).item())\n",
        "        return 0.5 * lam * sq_sum\n",
        "\n",
        "    def _batch_slice(self, x: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Optionally subsample the batch to accelerate trial evaluations, without changing randomness.\"\"\"\n",
        "        if self.eval_fraction >= 1.0:\n",
        "            return x, y\n",
        "        n = x.shape[0]\n",
        "        m = max(1, int(n * self.eval_fraction))\n",
        "        return x[:m], y[:m]\n",
        "\n",
        "    def step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: torch.Tensor,\n",
        "        targets: torch.Tensor,\n",
        "        criterion: nn.Module,\n",
        "    ) -> Tuple[float, float, int]:\n",
        "        \"\"\"Perform one optimizer step using line search.\n",
        "\n",
        "        Returns:\n",
        "            loss_value: float (full-batch loss before the update)\n",
        "            accepted_lr: float (lr used for the accepted step)\n",
        "            num_evals: int (number of trial forward evals performed for acceptance)\n",
        "        \"\"\"\n",
        "        model.train()\n",
        "\n",
        "        # 1) Forward/backward at w0 to get L0 and grads\n",
        "        self.opt.zero_grad(set_to_none=True)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss_value = float(loss.item())\n",
        "        loss.backward()\n",
        "\n",
        "        # Snapshot grads in a deterministic order\n",
        "        params: List[torch.nn.Parameter] = [p for p in model.parameters() if p.requires_grad]\n",
        "        saved_grads: List[torch.Tensor] = [p.grad.detach().clone() if p.grad is not None else torch.zeros_like(p) for p in params]\n",
        "\n",
        "        # Snapshot initial weights and optimizer state\n",
        "        w0_state = copy.deepcopy(model.state_dict())\n",
        "        opt0_state = copy.deepcopy(self.opt.state_dict())\n",
        "\n",
        "        # Helper to restore to w0 and reset grads\n",
        "        def restore_w0_and_grads():\n",
        "            model.load_state_dict(w0_state)\n",
        "            self.opt.load_state_dict(opt0_state)\n",
        "            for p, g in zip(params, saved_grads):\n",
        "                p.grad = g.clone()\n",
        "\n",
        "        # Compute L0 (optionally + L2) on the eval slice without grad\n",
        "        with torch.no_grad():\n",
        "            xe, ye = self._batch_slice(inputs, targets)\n",
        "            L0_eval = float(criterion(model(xe), ye).item()) + self._l2_weight_decay_term(model)\n",
        "\n",
        "        num_evals = 0\n",
        "        best = None  # (lr, loss_eval, model_state, opt_state)\n",
        "\n",
        "        # ---------- Stage A: try to GROW from prev lr ----------\n",
        "        lr_try = float(min(max(self.state.prev_lr, self.lr_min), self.lr_max))\n",
        "        passes_any = False\n",
        "        for _ in range(self.max_grow):\n",
        "            restore_w0_and_grads()\n",
        "            set_all_lrs(self.opt, lr_try)\n",
        "            # Apply one update with the stored grads\n",
        "            self.opt.step()\n",
        "\n",
        "            # Evaluate acceptance condition\n",
        "            with torch.no_grad():\n",
        "                xe, ye = self._batch_slice(inputs, targets)\n",
        "                L_new = float(criterion(model(xe), ye).item()) + self._l2_weight_decay_term(model)\n",
        "            num_evals += 1\n",
        "\n",
        "            # Compute g^T Δw using saved grads and delta weights\n",
        "            gtdw = 0.0\n",
        "            for p, g, name in zip(params, saved_grads, range(len(params))):\n",
        "                # Δw = w_new - w0\n",
        "                dw = p.detach() - w0_state[list(w0_state.keys())[name]].to(p.device)\n",
        "                gtdw += float(torch.sum(g * dw).item())\n",
        "\n",
        "            # Armijo sufficient decrease: L(w + Δw) <= L(w) + c * g^T Δw\n",
        "            # Note: g^T Δw is typically negative; RHS < L0_eval when c in (0,1).\n",
        "            if L_new <= L0_eval + self.c * gtdw:\n",
        "                passes_any = True\n",
        "                best = (lr_try, L_new, copy.deepcopy(model.state_dict()), copy.deepcopy(self.opt.state_dict()))\n",
        "                # attempt to grow further\n",
        "                lr_try = min(lr_try * self.growth, self.lr_max)\n",
        "            else:\n",
        "                # first failure → stop growing\n",
        "                break\n",
        "\n",
        "        if not passes_any:\n",
        "            # ---------- Stage B: BACKTRACK from prev lr ----------\n",
        "            lr_try = max(self.state.prev_lr * self.shrink, self.lr_min)\n",
        "            for _ in range(self.max_backtrack):\n",
        "                restore_w0_and_grads()\n",
        "                set_all_lrs(self.opt, lr_try)\n",
        "                self.opt.step()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    xe, ye = self._batch_slice(inputs, targets)\n",
        "                    L_new = float(criterion(model(xe), ye).item()) + self._l2_weight_decay_term(model)\n",
        "                num_evals += 1\n",
        "\n",
        "                # Compute g^T Δw\n",
        "                gtdw = 0.0\n",
        "                for p, g, name in zip(params, saved_grads, range(len(params))):\n",
        "                    dw = p.detach() - w0_state[list(w0_state.keys())[name]].to(p.device)\n",
        "                    gtdw += float(torch.sum(g * dw).item())\n",
        "\n",
        "                if L_new <= L0_eval + self.c * gtdw:\n",
        "                    best = (lr_try, L_new, copy.deepcopy(model.state_dict()), copy.deepcopy(self.opt.state_dict()))\n",
        "                    passes_any = True\n",
        "                    break\n",
        "                lr_try = max(lr_try * self.shrink, self.lr_min)\n",
        "\n",
        "        # ---------- Finalize ----------\n",
        "        if passes_any and best is not None:\n",
        "            accepted_lr, _, best_w, best_opt = best\n",
        "            model.load_state_dict(best_w)\n",
        "            self.opt.load_state_dict(best_opt)\n",
        "            self.state.accepted_lr = float(accepted_lr)\n",
        "            # modest optimism for next step\n",
        "            self.state.prev_lr = float(min(accepted_lr * self.growth, self.lr_max))\n",
        "        else:\n",
        "            # No step satisfied Armijo; we accept NO UPDATE and reduce prev lr.\n",
        "            restore_w0_and_grads()\n",
        "            # (optional) perform the smallest step if it strictly reduces loss; disabled for safety\n",
        "            self.state.prev_lr = float(max(self.state.prev_lr * self.shrink, self.lr_min))\n",
        "            self.state.accepted_lr = 0.0  # signal skipped\n",
        "\n",
        "        # Book-keeping\n",
        "        self.state.ema_lr = self.state.ema_lr * self.state.ema_beta + (1 - self.state.ema_beta) * max(self.state.accepted_lr, 0.0)\n",
        "        self.state.last_num_evals = num_evals\n",
        "\n",
        "        # Return the full-batch *pre-update* loss (for logging), accepted lr, and eval count\n",
        "        return loss_value, float(self.state.accepted_lr), num_evals\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3) # Nominal LR, will be overridden by LS\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize LineSearchMuon controller\n",
        "ls_ctl = LineSearchMuon(\n",
        "    optimizer,\n",
        "    init_lr=2e-2,\n",
        "    c=0.2,\n",
        "    growth=1.4,\n",
        "    shrink=0.5,\n",
        "    lr_min=5e-6,\n",
        "    lr_max=1.0,\n",
        "    max_grow=3,\n",
        "    max_backtrack=6,\n",
        "    eval_fraction=1.0,\n",
        ")\n",
        "\n",
        "print(\"Optimizer, criterion, and LineSearchMuon controller initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizer, criterion, and LineSearchMuon controller initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aecd6ce"
      },
      "source": [
        "## Define Evaluation Function for CIFAR-10\n",
        "\n",
        "### Subtask:\n",
        "Create an `evaluate` function that calculates the loss and accuracy on the CIFAR-10 test dataset, to be used periodically during training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4095def8"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `evaluate` function as per the instructions, which involves setting the model to evaluation mode, iterating through the test dataset, calculating loss and accuracy, and then setting the model back to training mode.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbd5e6c5",
        "outputId": "a968c900-5217-4edc-fde3-82bc239508d7"
      },
      "source": [
        "def evaluate():\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs = inputs.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item() * targets.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_correct += (predicted == targets).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    model.train() # Set model back to training mode\n",
        "    return average_loss, accuracy\n",
        "\n",
        "print(\"Evaluation function 'evaluate' defined.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation function 'evaluate' defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64780548"
      },
      "source": [
        "## Execute Training Loop (200 Epochs)\n",
        "\n",
        "### Subtask:\n",
        "Run the training loop for 200 epochs using the `train_one_epoch_ls` function, incorporating the CIFAR-10 data, MLP model, line search controller, and criterion. Log training progress and validate periodically.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27747527"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully initialized the model, data loaders, optimizer, criterion, LineSearchMuon controller, and the evaluation function. Now, I need to define the `train_one_epoch_ls` function, which is critical for the training loop and was provided in an earlier notebook cell, but not yet explicitly defined in the current working context. After defining this function, I will implement the main training loop as described in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "946b60a9",
        "outputId": "5221b95b-2500-44fc-8657-4ff8245d1a8c"
      },
      "source": [
        "def train_one_epoch_ls(\n",
        "    model: nn.Module,\n",
        "    ls_ctl: LineSearchMuon,\n",
        "    trainloader: Iterable[Tuple[torch.Tensor, torch.Tensor]],\n",
        "    device: torch.device,\n",
        "    criterion: nn.Module,\n",
        "    log_every: int = 100,\n",
        ") -> Tuple[float, float, float]:\n",
        "    \"\"\"Train for exactly one epoch using the line-search controller.\n",
        "\n",
        "    Returns:\n",
        "        avg_loss, avg_acc, avg_accepted_lr\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    lr_accum = 0.0\n",
        "    steps = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(trainloader, 1):\n",
        "        inputs = inputs.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        # one LS-controlled step\n",
        "        loss_val, accepted_lr, _ = ls_ctl.step(model, inputs, targets, criterion)\n",
        "\n",
        "        # compute batch accuracy from the forward we already did (do another forward cheaply)\n",
        "        with torch.no_grad():\n",
        "            logits = model(inputs)\n",
        "            preds = logits.argmax(1)\n",
        "            running_correct += int((preds == targets).sum().item())\n",
        "            running_total += int(targets.size(0))\n",
        "\n",
        "        running_loss += loss_val * targets.size(0)\n",
        "        lr_accum += max(accepted_lr, 0.0)\n",
        "        steps += 1\n",
        "\n",
        "        if (i % log_every) == 0:\n",
        "            avg_tr_loss = running_loss / max(1, running_total)\n",
        "            avg_tr_acc = 100.0 * running_correct / max(1, running_total)\n",
        "            avg_lr = lr_accum / max(1, steps)\n",
        "            print(f\"[LS] step {i:04d}/{len(trainloader)} | loss {avg_tr_loss:.4f} | acc {avg_tr_acc:5.2f}% | \"\n",
        "                  f\"lr~{avg_lr:.3g} | evals/step~{ls_ctl.state.last_num_evals}\")\n",
        "\n",
        "    avg_loss = running_loss / max(1, running_total)\n",
        "    avg_acc = running_correct / max(1, running_total)\n",
        "    avg_lr = lr_accum / max(1, steps) if steps > 0 else 0.0\n",
        "    return float(avg_loss), float(avg_acc), float(avg_lr)\n",
        "\n",
        "# Define training parameters\n",
        "epochs = 200\n",
        "save_path = 'best_model.pth'\n",
        "best = 0.0 # Best validation accuracy\n",
        "\n",
        "print(f\"Starting training for {epochs} epochs...\")\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    tr_loss, tr_acc, tr_lr = train_one_epoch_ls(model, ls_ctl, trainloader, device, criterion, log_every=100)\n",
        "\n",
        "    val_loss, val_acc = evaluate()\n",
        "    print(f\"[Eval] epoch {epoch:03d}/{epochs} | Tr Loss {tr_loss:.4f} | Tr Acc {tr_acc*100:.2f}% | Val Loss {val_loss:.4f} | Val Acc {val_acc*100:.2f}% (lr~{tr_lr:.3g})\")\n",
        "\n",
        "    if val_acc > best:\n",
        "        best = val_acc\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"New best model saved with validation accuracy: {best*100:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining complete.\")\n",
        "print(f\"Best validation accuracy achieved: {best*100:.2f}%\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 200 epochs...\n",
            "[LS] step 0100/196 | loss 2.0143 | acc 29.05% | lr~0.0114 | evals/step~2\n",
            "[Eval] epoch 001/200 | Tr Loss 2.0081 | Tr Acc 27.89% | Val Loss 1.9447 | Val Acc 26.83% (lr~0.0147)\n",
            "New best model saved with validation accuracy: 26.83%\n",
            "[LS] step 0100/196 | loss 1.8873 | acc 29.96% | lr~0.0094 | evals/step~3\n",
            "[Eval] epoch 002/200 | Tr Loss 1.8721 | Tr Acc 30.69% | Val Loss 1.8561 | Val Acc 30.45% (lr~0.0076)\n",
            "New best model saved with validation accuracy: 30.45%\n",
            "[LS] step 0100/196 | loss 1.8565 | acc 31.41% | lr~0.0106 | evals/step~2\n",
            "[Eval] epoch 003/200 | Tr Loss 1.8520 | Tr Acc 31.21% | Val Loss 1.9164 | Val Acc 26.94% (lr~0.0096)\n",
            "[LS] step 0100/196 | loss 1.8779 | acc 30.65% | lr~0.0147 | evals/step~2\n",
            "[Eval] epoch 004/200 | Tr Loss 1.8631 | Tr Acc 30.82% | Val Loss 1.8355 | Val Acc 29.55% (lr~0.0109)\n",
            "[LS] step 0100/196 | loss 1.8514 | acc 29.54% | lr~0.00943 | evals/step~3\n",
            "[Eval] epoch 005/200 | Tr Loss 1.8392 | Tr Acc 30.03% | Val Loss 1.8722 | Val Acc 28.06% (lr~0.00877)\n",
            "[LS] step 0100/196 | loss 1.7973 | acc 32.17% | lr~0.0065 | evals/step~3\n",
            "[Eval] epoch 006/200 | Tr Loss 1.8224 | Tr Acc 31.26% | Val Loss 1.9156 | Val Acc 27.23% (lr~0.00963)\n",
            "[LS] step 0100/196 | loss 1.8168 | acc 31.72% | lr~0.0115 | evals/step~3\n",
            "[Eval] epoch 007/200 | Tr Loss 1.8115 | Tr Acc 32.11% | Val Loss 1.9250 | Val Acc 28.81% (lr~0.0105)\n",
            "[LS] step 0100/196 | loss 1.8122 | acc 33.16% | lr~0.011 | evals/step~2\n",
            "[Eval] epoch 008/200 | Tr Loss 1.8073 | Tr Acc 33.29% | Val Loss 1.8783 | Val Acc 29.66% (lr~0.0113)\n",
            "[LS] step 0100/196 | loss 1.8153 | acc 33.04% | lr~0.0131 | evals/step~3\n",
            "[Eval] epoch 009/200 | Tr Loss 1.8022 | Tr Acc 33.03% | Val Loss 1.7808 | Val Acc 33.05% (lr~0.0107)\n",
            "New best model saved with validation accuracy: 33.05%\n",
            "[LS] step 0100/196 | loss 1.7418 | acc 34.83% | lr~0.00578 | evals/step~2\n",
            "[Eval] epoch 010/200 | Tr Loss 1.7525 | Tr Acc 34.58% | Val Loss 1.8484 | Val Acc 31.65% (lr~0.00812)\n",
            "[LS] step 0100/196 | loss 1.7782 | acc 34.28% | lr~0.0109 | evals/step~2\n",
            "[Eval] epoch 011/200 | Tr Loss 1.7786 | Tr Acc 34.20% | Val Loss 1.9393 | Val Acc 28.17% (lr~0.012)\n",
            "[LS] step 0100/196 | loss 1.7748 | acc 33.12% | lr~0.0112 | evals/step~3\n",
            "[Eval] epoch 012/200 | Tr Loss 1.7659 | Tr Acc 33.51% | Val Loss 1.9027 | Val Acc 28.10% (lr~0.0108)\n",
            "[LS] step 0100/196 | loss 1.7494 | acc 34.23% | lr~0.0125 | evals/step~3\n",
            "[Eval] epoch 013/200 | Tr Loss 1.7418 | Tr Acc 34.32% | Val Loss 1.7505 | Val Acc 34.16% (lr~0.00984)\n",
            "New best model saved with validation accuracy: 34.16%\n",
            "[LS] step 0100/196 | loss 1.7055 | acc 35.73% | lr~0.0086 | evals/step~3\n",
            "[Eval] epoch 014/200 | Tr Loss 1.7146 | Tr Acc 36.07% | Val Loss 2.3257 | Val Acc 29.11% (lr~0.0103)\n",
            "[LS] step 0100/196 | loss 1.7350 | acc 35.09% | lr~0.0109 | evals/step~3\n",
            "[Eval] epoch 015/200 | Tr Loss 1.7233 | Tr Acc 34.98% | Val Loss 1.8225 | Val Acc 31.26% (lr~0.00935)\n",
            "[LS] step 0100/196 | loss 1.7099 | acc 34.81% | lr~0.0088 | evals/step~3\n",
            "[Eval] epoch 016/200 | Tr Loss 1.7004 | Tr Acc 35.81% | Val Loss 1.7549 | Val Acc 35.28% (lr~0.00971)\n",
            "New best model saved with validation accuracy: 35.28%\n",
            "[LS] step 0100/196 | loss 1.7006 | acc 36.20% | lr~0.0102 | evals/step~3\n",
            "[Eval] epoch 017/200 | Tr Loss 1.6944 | Tr Acc 36.21% | Val Loss 1.7197 | Val Acc 34.55% (lr~0.00831)\n",
            "[LS] step 0100/196 | loss 1.6789 | acc 36.25% | lr~0.00962 | evals/step~3\n",
            "[Eval] epoch 018/200 | Tr Loss 1.6813 | Tr Acc 35.74% | Val Loss 1.7808 | Val Acc 31.81% (lr~0.0104)\n",
            "[LS] step 0100/196 | loss 1.6772 | acc 35.70% | lr~0.0125 | evals/step~2\n",
            "[Eval] epoch 019/200 | Tr Loss 1.6788 | Tr Acc 35.98% | Val Loss 1.7823 | Val Acc 33.08% (lr~0.0126)\n",
            "[LS] step 0100/196 | loss 1.6754 | acc 36.19% | lr~0.0146 | evals/step~2\n",
            "[Eval] epoch 020/200 | Tr Loss 1.6767 | Tr Acc 36.43% | Val Loss 1.7233 | Val Acc 35.37% (lr~0.0135)\n",
            "New best model saved with validation accuracy: 35.37%\n",
            "[LS] step 0100/196 | loss 1.6891 | acc 35.98% | lr~0.0129 | evals/step~2\n",
            "[Eval] epoch 021/200 | Tr Loss 1.7005 | Tr Acc 35.63% | Val Loss 1.8164 | Val Acc 32.50% (lr~0.0124)\n",
            "[LS] step 0100/196 | loss 1.6966 | acc 35.85% | lr~0.0113 | evals/step~2\n",
            "[Eval] epoch 022/200 | Tr Loss 1.7104 | Tr Acc 35.07% | Val Loss 1.7873 | Val Acc 32.89% (lr~0.00945)\n",
            "[LS] step 0100/196 | loss 1.7117 | acc 34.63% | lr~0.00996 | evals/step~3\n",
            "[Eval] epoch 023/200 | Tr Loss 1.7130 | Tr Acc 34.77% | Val Loss 1.7914 | Val Acc 32.61% (lr~0.00933)\n",
            "[LS] step 0100/196 | loss 1.6942 | acc 35.41% | lr~0.0112 | evals/step~3\n",
            "[Eval] epoch 024/200 | Tr Loss 1.6970 | Tr Acc 35.84% | Val Loss 1.9197 | Val Acc 31.79% (lr~0.0129)\n",
            "[LS] step 0100/196 | loss 1.6946 | acc 35.75% | lr~0.00891 | evals/step~2\n",
            "[Eval] epoch 025/200 | Tr Loss 1.6910 | Tr Acc 35.69% | Val Loss 1.8035 | Val Acc 31.33% (lr~0.00979)\n",
            "[LS] step 0100/196 | loss 1.7482 | acc 33.24% | lr~0.0152 | evals/step~2\n",
            "[Eval] epoch 026/200 | Tr Loss 1.7226 | Tr Acc 34.28% | Val Loss 1.8028 | Val Acc 32.26% (lr~0.0132)\n",
            "[LS] step 0100/196 | loss 1.6678 | acc 35.97% | lr~0.00817 | evals/step~3\n",
            "[Eval] epoch 027/200 | Tr Loss 1.6778 | Tr Acc 35.71% | Val Loss 1.7622 | Val Acc 33.46% (lr~0.0102)\n",
            "[LS] step 0100/196 | loss 1.6652 | acc 36.51% | lr~0.00905 | evals/step~2\n",
            "[Eval] epoch 028/200 | Tr Loss 1.6640 | Tr Acc 36.63% | Val Loss 1.8342 | Val Acc 31.36% (lr~0.0089)\n",
            "[LS] step 0100/196 | loss 1.7093 | acc 35.24% | lr~0.0128 | evals/step~2\n",
            "[Eval] epoch 029/200 | Tr Loss 1.6906 | Tr Acc 35.84% | Val Loss 1.7965 | Val Acc 32.23% (lr~0.0109)\n",
            "[LS] step 0100/196 | loss 1.6611 | acc 37.02% | lr~0.0128 | evals/step~2\n",
            "[Eval] epoch 030/200 | Tr Loss 1.6816 | Tr Acc 36.31% | Val Loss 1.8891 | Val Acc 29.61% (lr~0.0139)\n",
            "[LS] step 0100/196 | loss 1.7031 | acc 34.94% | lr~0.0092 | evals/step~3\n",
            "[Eval] epoch 031/200 | Tr Loss 1.6950 | Tr Acc 35.12% | Val Loss 1.8452 | Val Acc 31.49% (lr~0.00879)\n",
            "[LS] step 0100/196 | loss 1.6789 | acc 36.00% | lr~0.00888 | evals/step~3\n",
            "[Eval] epoch 032/200 | Tr Loss 1.6891 | Tr Acc 36.10% | Val Loss 1.8509 | Val Acc 32.00% (lr~0.0103)\n",
            "[LS] step 0100/196 | loss 1.6905 | acc 35.41% | lr~0.00759 | evals/step~2\n",
            "[Eval] epoch 033/200 | Tr Loss 1.6815 | Tr Acc 35.92% | Val Loss 1.7960 | Val Acc 34.33% (lr~0.00694)\n",
            "[LS] step 0100/196 | loss 1.6852 | acc 36.04% | lr~0.0133 | evals/step~2\n",
            "[Eval] epoch 034/200 | Tr Loss 1.6817 | Tr Acc 36.20% | Val Loss 1.8983 | Val Acc 31.64% (lr~0.0134)\n",
            "[LS] step 0100/196 | loss 1.6817 | acc 35.94% | lr~0.0112 | evals/step~2\n",
            "[Eval] epoch 035/200 | Tr Loss 1.6951 | Tr Acc 36.05% | Val Loss 1.7920 | Val Acc 34.12% (lr~0.0114)\n",
            "[LS] step 0100/196 | loss 1.6979 | acc 35.89% | lr~0.00545 | evals/step~5\n",
            "[Eval] epoch 036/200 | Tr Loss 1.6947 | Tr Acc 36.37% | Val Loss 1.8712 | Val Acc 31.13% (lr~0.0065)\n",
            "[LS] step 0100/196 | loss 1.6834 | acc 36.78% | lr~0.0105 | evals/step~2\n",
            "[Eval] epoch 037/200 | Tr Loss 1.6746 | Tr Acc 36.87% | Val Loss 1.8023 | Val Acc 34.08% (lr~0.0105)\n",
            "[LS] step 0100/196 | loss 1.6800 | acc 37.71% | lr~0.0084 | evals/step~3\n",
            "[Eval] epoch 038/200 | Tr Loss 1.6738 | Tr Acc 37.50% | Val Loss 1.7655 | Val Acc 35.47% (lr~0.0072)\n",
            "New best model saved with validation accuracy: 35.47%\n",
            "[LS] step 0100/196 | loss 1.6466 | acc 38.37% | lr~0.0092 | evals/step~3\n",
            "[Eval] epoch 039/200 | Tr Loss 1.6616 | Tr Acc 38.03% | Val Loss 1.9056 | Val Acc 31.62% (lr~0.0113)\n",
            "[LS] step 0100/196 | loss 1.6843 | acc 35.89% | lr~0.0159 | evals/step~3\n",
            "[Eval] epoch 040/200 | Tr Loss 1.6779 | Tr Acc 36.50% | Val Loss 1.8041 | Val Acc 34.57% (lr~0.0144)\n",
            "[LS] step 0100/196 | loss 1.6592 | acc 37.50% | lr~0.011 | evals/step~3\n",
            "[Eval] epoch 041/200 | Tr Loss 1.6569 | Tr Acc 37.81% | Val Loss 1.8180 | Val Acc 33.92% (lr~0.0129)\n",
            "[LS] step 0100/196 | loss 1.6666 | acc 37.28% | lr~0.0149 | evals/step~3\n",
            "[Eval] epoch 042/200 | Tr Loss 1.6678 | Tr Acc 37.30% | Val Loss 1.8173 | Val Acc 34.13% (lr~0.0128)\n",
            "[LS] step 0100/196 | loss 1.6693 | acc 37.15% | lr~0.00995 | evals/step~3\n",
            "[Eval] epoch 043/200 | Tr Loss 1.6703 | Tr Acc 37.10% | Val Loss 1.8051 | Val Acc 35.29% (lr~0.00894)\n",
            "[LS] step 0100/196 | loss 1.6599 | acc 37.70% | lr~0.00631 | evals/step~2\n",
            "[Eval] epoch 044/200 | Tr Loss 1.6612 | Tr Acc 37.73% | Val Loss 1.7998 | Val Acc 33.10% (lr~0.00699)\n",
            "[LS] step 0100/196 | loss 1.6475 | acc 36.27% | lr~0.00856 | evals/step~3\n",
            "[Eval] epoch 045/200 | Tr Loss 1.6652 | Tr Acc 35.91% | Val Loss 1.8918 | Val Acc 30.02% (lr~0.011)\n",
            "[LS] step 0100/196 | loss 1.7154 | acc 35.25% | lr~0.0135 | evals/step~2\n",
            "[Eval] epoch 046/200 | Tr Loss 1.7006 | Tr Acc 35.51% | Val Loss 1.8256 | Val Acc 33.17% (lr~0.0134)\n",
            "[LS] step 0100/196 | loss 1.6709 | acc 36.67% | lr~0.0109 | evals/step~3\n",
            "[Eval] epoch 047/200 | Tr Loss 1.6705 | Tr Acc 37.02% | Val Loss 1.9133 | Val Acc 32.97% (lr~0.009)\n",
            "[LS] step 0100/196 | loss 1.6451 | acc 38.03% | lr~0.00935 | evals/step~2\n",
            "[Eval] epoch 048/200 | Tr Loss 1.6388 | Tr Acc 38.36% | Val Loss 1.8264 | Val Acc 34.92% (lr~0.00866)\n",
            "[LS] step 0100/196 | loss 1.6046 | acc 39.46% | lr~0.00613 | evals/step~3\n",
            "[Eval] epoch 049/200 | Tr Loss 1.6301 | Tr Acc 38.52% | Val Loss 1.7890 | Val Acc 33.02% (lr~0.0106)\n",
            "[LS] step 0100/196 | loss 1.6731 | acc 36.93% | lr~0.016 | evals/step~3\n",
            "[Eval] epoch 050/200 | Tr Loss 1.6644 | Tr Acc 37.18% | Val Loss 1.8751 | Val Acc 32.05% (lr~0.014)\n",
            "[LS] step 0100/196 | loss 1.6045 | acc 38.35% | lr~0.00661 | evals/step~7\n",
            "[Eval] epoch 051/200 | Tr Loss 1.6233 | Tr Acc 37.49% | Val Loss 1.8185 | Val Acc 33.55% (lr~0.00665)\n",
            "[LS] step 0100/196 | loss 1.6194 | acc 37.86% | lr~0.00715 | evals/step~3\n",
            "[Eval] epoch 052/200 | Tr Loss 1.6263 | Tr Acc 37.54% | Val Loss 1.8989 | Val Acc 31.04% (lr~0.00945)\n",
            "[LS] step 0100/196 | loss 1.6857 | acc 35.66% | lr~0.015 | evals/step~3\n",
            "[Eval] epoch 053/200 | Tr Loss 1.6692 | Tr Acc 35.83% | Val Loss 1.8036 | Val Acc 32.83% (lr~0.0115)\n",
            "[LS] step 0100/196 | loss 1.6300 | acc 36.96% | lr~0.0141 | evals/step~2\n",
            "[Eval] epoch 054/200 | Tr Loss 1.6474 | Tr Acc 36.12% | Val Loss 1.9318 | Val Acc 30.95% (lr~0.0129)\n",
            "[LS] step 0100/196 | loss 1.6336 | acc 35.99% | lr~0.00507 | evals/step~4\n",
            "[Eval] epoch 055/200 | Tr Loss 1.6369 | Tr Acc 36.31% | Val Loss 1.8764 | Val Acc 32.40% (lr~0.00645)\n",
            "[LS] step 0100/196 | loss 1.6535 | acc 36.64% | lr~0.0151 | evals/step~3\n",
            "[Eval] epoch 056/200 | Tr Loss 1.6532 | Tr Acc 36.60% | Val Loss 1.9525 | Val Acc 29.79% (lr~0.0123)\n",
            "[LS] step 0100/196 | loss 1.6855 | acc 35.02% | lr~0.011 | evals/step~2\n",
            "[Eval] epoch 057/200 | Tr Loss 1.6711 | Tr Acc 35.39% | Val Loss 1.8898 | Val Acc 31.83% (lr~0.0115)\n",
            "[LS] step 0100/196 | loss 1.6350 | acc 36.12% | lr~0.00843 | evals/step~3\n",
            "[Eval] epoch 058/200 | Tr Loss 1.6373 | Tr Acc 35.97% | Val Loss 1.9627 | Val Acc 31.13% (lr~0.00913)\n",
            "[LS] step 0100/196 | loss 1.6413 | acc 36.07% | lr~0.00828 | evals/step~2\n",
            "[Eval] epoch 059/200 | Tr Loss 1.6482 | Tr Acc 35.93% | Val Loss 1.9129 | Val Acc 30.33% (lr~0.00953)\n",
            "[LS] step 0100/196 | loss 1.6261 | acc 37.22% | lr~0.00999 | evals/step~7\n",
            "[Eval] epoch 060/200 | Tr Loss 1.6349 | Tr Acc 36.86% | Val Loss 2.0641 | Val Acc 30.49% (lr~0.00878)\n",
            "[LS] step 0100/196 | loss 1.6893 | acc 34.64% | lr~0.0136 | evals/step~4\n",
            "[Eval] epoch 061/200 | Tr Loss 1.6779 | Tr Acc 35.00% | Val Loss 1.8811 | Val Acc 30.02% (lr~0.013)\n",
            "[LS] step 0100/196 | loss 1.6599 | acc 36.33% | lr~0.0107 | evals/step~3\n",
            "[Eval] epoch 062/200 | Tr Loss 1.6476 | Tr Acc 36.84% | Val Loss 1.8138 | Val Acc 32.75% (lr~0.0097)\n",
            "[LS] step 0100/196 | loss 1.6213 | acc 37.03% | lr~0.01 | evals/step~7\n",
            "[Eval] epoch 063/200 | Tr Loss 1.6290 | Tr Acc 36.87% | Val Loss 1.8979 | Val Acc 29.83% (lr~0.0122)\n",
            "[LS] step 0100/196 | loss 1.6217 | acc 37.32% | lr~0.0102 | evals/step~2\n",
            "[Eval] epoch 064/200 | Tr Loss 1.6322 | Tr Acc 36.72% | Val Loss 1.8512 | Val Acc 29.99% (lr~0.0116)\n",
            "[LS] step 0100/196 | loss 1.6203 | acc 36.92% | lr~0.0106 | evals/step~2\n",
            "[Eval] epoch 065/200 | Tr Loss 1.6189 | Tr Acc 36.86% | Val Loss 1.8999 | Val Acc 31.92% (lr~0.00961)\n",
            "[LS] step 0100/196 | loss 1.6669 | acc 36.28% | lr~0.00942 | evals/step~2\n",
            "[Eval] epoch 066/200 | Tr Loss 1.6486 | Tr Acc 36.71% | Val Loss 1.8713 | Val Acc 32.02% (lr~0.00906)\n",
            "[LS] step 0100/196 | loss 1.6353 | acc 36.70% | lr~0.0115 | evals/step~3\n",
            "[Eval] epoch 067/200 | Tr Loss 1.6372 | Tr Acc 36.40% | Val Loss 1.8783 | Val Acc 31.41% (lr~0.00881)\n",
            "[LS] step 0100/196 | loss 1.6138 | acc 36.71% | lr~0.00907 | evals/step~3\n",
            "[Eval] epoch 068/200 | Tr Loss 1.6071 | Tr Acc 37.00% | Val Loss 1.8504 | Val Acc 32.45% (lr~0.0082)\n",
            "[LS] step 0100/196 | loss 1.6040 | acc 37.29% | lr~0.00959 | evals/step~3\n",
            "[Eval] epoch 069/200 | Tr Loss 1.6014 | Tr Acc 37.52% | Val Loss 1.8947 | Val Acc 33.34% (lr~0.00826)\n",
            "[LS] step 0100/196 | loss 1.5993 | acc 37.69% | lr~0.0131 | evals/step~2\n",
            "[Eval] epoch 070/200 | Tr Loss 1.6067 | Tr Acc 37.20% | Val Loss 1.9958 | Val Acc 31.78% (lr~0.00987)\n",
            "[LS] step 0100/196 | loss 1.5882 | acc 37.95% | lr~0.00805 | evals/step~3\n",
            "[Eval] epoch 071/200 | Tr Loss 1.5931 | Tr Acc 37.64% | Val Loss 1.9719 | Val Acc 32.08% (lr~0.00967)\n",
            "[LS] step 0100/196 | loss 1.5863 | acc 37.90% | lr~0.0111 | evals/step~2\n",
            "[Eval] epoch 072/200 | Tr Loss 1.6072 | Tr Acc 37.64% | Val Loss 2.0645 | Val Acc 30.84% (lr~0.0107)\n",
            "[LS] step 0100/196 | loss 1.6090 | acc 37.55% | lr~0.0075 | evals/step~3\n",
            "[Eval] epoch 073/200 | Tr Loss 1.6146 | Tr Acc 37.79% | Val Loss 1.8807 | Val Acc 32.54% (lr~0.00802)\n",
            "[LS] step 0100/196 | loss 1.6003 | acc 37.74% | lr~0.00959 | evals/step~2\n",
            "[Eval] epoch 074/200 | Tr Loss 1.5972 | Tr Acc 37.78% | Val Loss 1.8720 | Val Acc 32.17% (lr~0.00715)\n",
            "[LS] step 0100/196 | loss 1.5788 | acc 38.37% | lr~0.011 | evals/step~3\n",
            "[Eval] epoch 075/200 | Tr Loss 1.5854 | Tr Acc 37.94% | Val Loss 1.9581 | Val Acc 31.88% (lr~0.0103)\n",
            "[LS] step 0100/196 | loss 1.5825 | acc 38.59% | lr~0.00868 | evals/step~3\n",
            "[Eval] epoch 076/200 | Tr Loss 1.5608 | Tr Acc 39.18% | Val Loss 1.8645 | Val Acc 33.61% (lr~0.00678)\n",
            "[LS] step 0100/196 | loss 1.5430 | acc 40.34% | lr~0.00491 | evals/step~2\n",
            "[Eval] epoch 077/200 | Tr Loss 1.5553 | Tr Acc 39.81% | Val Loss 1.9169 | Val Acc 33.23% (lr~0.00571)\n",
            "[LS] step 0100/196 | loss 1.5626 | acc 39.06% | lr~0.00792 | evals/step~3\n",
            "[Eval] epoch 078/200 | Tr Loss 1.5727 | Tr Acc 38.95% | Val Loss 1.9142 | Val Acc 32.94% (lr~0.00813)\n",
            "[LS] step 0100/196 | loss 1.5775 | acc 39.54% | lr~0.0109 | evals/step~2\n",
            "[Eval] epoch 079/200 | Tr Loss 1.5780 | Tr Acc 39.27% | Val Loss 1.9139 | Val Acc 32.33% (lr~0.0104)\n",
            "[LS] step 0100/196 | loss 1.6095 | acc 38.12% | lr~0.0139 | evals/step~2\n",
            "[Eval] epoch 080/200 | Tr Loss 1.6214 | Tr Acc 37.55% | Val Loss 1.8906 | Val Acc 33.00% (lr~0.0127)\n",
            "[LS] step 0100/196 | loss 1.5908 | acc 38.18% | lr~0.00759 | evals/step~3\n",
            "[Eval] epoch 081/200 | Tr Loss 1.5850 | Tr Acc 38.49% | Val Loss 1.8692 | Val Acc 33.61% (lr~0.00682)\n",
            "[LS] step 0100/196 | loss 1.5633 | acc 39.55% | lr~0.00766 | evals/step~2\n",
            "[Eval] epoch 082/200 | Tr Loss 1.5601 | Tr Acc 39.61% | Val Loss 1.8457 | Val Acc 34.87% (lr~0.00652)\n",
            "[LS] step 0100/196 | loss 1.5398 | acc 39.93% | lr~0.00761 | evals/step~3\n",
            "[Eval] epoch 083/200 | Tr Loss 1.5536 | Tr Acc 39.60% | Val Loss 1.8889 | Val Acc 33.26% (lr~0.00799)\n",
            "[LS] step 0100/196 | loss 1.5847 | acc 39.57% | lr~0.0105 | evals/step~2\n",
            "[Eval] epoch 084/200 | Tr Loss 1.5622 | Tr Acc 39.79% | Val Loss 1.9079 | Val Acc 31.46% (lr~0.00869)\n",
            "[LS] step 0100/196 | loss 1.5909 | acc 39.10% | lr~0.0132 | evals/step~2\n",
            "[Eval] epoch 085/200 | Tr Loss 1.5876 | Tr Acc 39.15% | Val Loss 1.8581 | Val Acc 33.91% (lr~0.0112)\n",
            "[LS] step 0100/196 | loss 1.5677 | acc 39.70% | lr~0.0124 | evals/step~2\n",
            "[Eval] epoch 086/200 | Tr Loss 1.5633 | Tr Acc 40.02% | Val Loss 1.8738 | Val Acc 32.20% (lr~0.0119)\n",
            "[LS] step 0100/196 | loss 1.5582 | acc 40.08% | lr~0.0102 | evals/step~3\n",
            "[Eval] epoch 087/200 | Tr Loss 1.5479 | Tr Acc 40.31% | Val Loss 1.8787 | Val Acc 34.32% (lr~0.00805)\n",
            "[LS] step 0100/196 | loss 1.5394 | acc 40.79% | lr~0.00713 | evals/step~3\n",
            "[Eval] epoch 088/200 | Tr Loss 1.5595 | Tr Acc 40.23% | Val Loss 1.8918 | Val Acc 33.50% (lr~0.00955)\n",
            "[LS] step 0100/196 | loss 1.5705 | acc 39.41% | lr~0.0108 | evals/step~3\n",
            "[Eval] epoch 089/200 | Tr Loss 1.5713 | Tr Acc 39.31% | Val Loss 1.9494 | Val Acc 31.28% (lr~0.0116)\n",
            "[LS] step 0100/196 | loss 1.6136 | acc 37.83% | lr~0.0142 | evals/step~2\n",
            "[Eval] epoch 090/200 | Tr Loss 1.6072 | Tr Acc 38.12% | Val Loss 1.9466 | Val Acc 31.70% (lr~0.0122)\n",
            "[LS] step 0100/196 | loss 1.5744 | acc 38.05% | lr~0.00822 | evals/step~2\n",
            "[Eval] epoch 091/200 | Tr Loss 1.5826 | Tr Acc 38.60% | Val Loss 2.0270 | Val Acc 32.01% (lr~0.0119)\n",
            "[LS] step 0100/196 | loss 1.5801 | acc 39.48% | lr~0.013 | evals/step~3\n",
            "[Eval] epoch 092/200 | Tr Loss 1.5752 | Tr Acc 39.81% | Val Loss 1.8625 | Val Acc 33.06% (lr~0.0113)\n",
            "[LS] step 0100/196 | loss 1.5481 | acc 40.67% | lr~0.00907 | evals/step~3\n",
            "[Eval] epoch 093/200 | Tr Loss 1.5444 | Tr Acc 40.51% | Val Loss 1.9795 | Val Acc 33.54% (lr~0.00792)\n",
            "[LS] step 0100/196 | loss 1.5268 | acc 41.40% | lr~0.0101 | evals/step~2\n",
            "[Eval] epoch 094/200 | Tr Loss 1.5247 | Tr Acc 41.27% | Val Loss 1.8994 | Val Acc 34.77% (lr~0.00927)\n",
            "[LS] step 0100/196 | loss 1.5519 | acc 40.67% | lr~0.0118 | evals/step~3\n",
            "[Eval] epoch 095/200 | Tr Loss 1.5620 | Tr Acc 40.39% | Val Loss 2.0783 | Val Acc 33.45% (lr~0.0123)\n",
            "[LS] step 0100/196 | loss 1.5677 | acc 40.29% | lr~0.00723 | evals/step~3\n",
            "[Eval] epoch 096/200 | Tr Loss 1.5705 | Tr Acc 39.96% | Val Loss 1.9415 | Val Acc 32.15% (lr~0.0105)\n",
            "[LS] step 0100/196 | loss 1.5480 | acc 40.39% | lr~0.0108 | evals/step~3\n",
            "[Eval] epoch 097/200 | Tr Loss 1.5728 | Tr Acc 39.85% | Val Loss 1.9014 | Val Acc 32.75% (lr~0.0117)\n",
            "[LS] step 0100/196 | loss 1.5864 | acc 38.84% | lr~0.0147 | evals/step~2\n",
            "[Eval] epoch 098/200 | Tr Loss 1.5790 | Tr Acc 38.85% | Val Loss 1.9456 | Val Acc 32.99% (lr~0.0102)\n",
            "[LS] step 0100/196 | loss 1.5704 | acc 39.94% | lr~0.0134 | evals/step~3\n",
            "[Eval] epoch 099/200 | Tr Loss 1.5716 | Tr Acc 39.97% | Val Loss 1.9403 | Val Acc 33.16% (lr~0.0149)\n",
            "[LS] step 0100/196 | loss 1.5467 | acc 40.32% | lr~0.0109 | evals/step~2\n",
            "[Eval] epoch 100/200 | Tr Loss 1.5368 | Tr Acc 40.69% | Val Loss 1.9480 | Val Acc 31.25% (lr~0.0104)\n",
            "[LS] step 0100/196 | loss 1.5446 | acc 39.96% | lr~0.00937 | evals/step~3\n",
            "[Eval] epoch 101/200 | Tr Loss 1.5748 | Tr Acc 39.44% | Val Loss 1.9253 | Val Acc 33.91% (lr~0.00963)\n",
            "[LS] step 0100/196 | loss 1.5371 | acc 40.90% | lr~0.0108 | evals/step~3\n",
            "[Eval] epoch 102/200 | Tr Loss 1.5654 | Tr Acc 40.00% | Val Loss 1.9701 | Val Acc 33.20% (lr~0.0106)\n",
            "[LS] step 0100/196 | loss 1.5705 | acc 39.55% | lr~0.00959 | evals/step~3\n",
            "[Eval] epoch 103/200 | Tr Loss 1.5642 | Tr Acc 39.81% | Val Loss 1.9003 | Val Acc 34.43% (lr~0.0098)\n",
            "[LS] step 0100/196 | loss 1.5174 | acc 41.79% | lr~0.0122 | evals/step~2\n",
            "[Eval] epoch 104/200 | Tr Loss 1.5196 | Tr Acc 41.59% | Val Loss 1.9043 | Val Acc 33.42% (lr~0.0116)\n",
            "[LS] step 0100/196 | loss 1.5132 | acc 41.32% | lr~0.00976 | evals/step~3\n",
            "[Eval] epoch 105/200 | Tr Loss 1.5201 | Tr Acc 41.33% | Val Loss 2.0489 | Val Acc 33.17% (lr~0.00983)\n",
            "[LS] step 0100/196 | loss 1.5766 | acc 40.72% | lr~0.00886 | evals/step~3\n",
            "[Eval] epoch 106/200 | Tr Loss 1.5683 | Tr Acc 40.51% | Val Loss 1.8820 | Val Acc 32.33% (lr~0.0108)\n",
            "[LS] step 0100/196 | loss 1.6301 | acc 38.81% | lr~0.0161 | evals/step~2\n",
            "[Eval] epoch 107/200 | Tr Loss 1.5930 | Tr Acc 39.74% | Val Loss 1.9032 | Val Acc 33.52% (lr~0.0146)\n",
            "[LS] step 0100/196 | loss 1.5413 | acc 40.87% | lr~0.0198 | evals/step~3\n",
            "[Eval] epoch 108/200 | Tr Loss 1.5474 | Tr Acc 40.67% | Val Loss 1.8574 | Val Acc 33.69% (lr~0.016)\n",
            "[LS] step 0100/196 | loss 1.5175 | acc 41.09% | lr~0.00743 | evals/step~3\n",
            "[Eval] epoch 109/200 | Tr Loss 1.5246 | Tr Acc 40.70% | Val Loss 2.0269 | Val Acc 33.44% (lr~0.00657)\n",
            "[LS] step 0100/196 | loss 1.5278 | acc 41.12% | lr~0.0121 | evals/step~2\n",
            "[Eval] epoch 110/200 | Tr Loss 1.5259 | Tr Acc 40.91% | Val Loss 1.9636 | Val Acc 33.42% (lr~0.0104)\n",
            "[LS] step 0100/196 | loss 1.4935 | acc 41.52% | lr~0.0066 | evals/step~2\n",
            "[Eval] epoch 111/200 | Tr Loss 1.4972 | Tr Acc 41.90% | Val Loss 1.9954 | Val Acc 33.92% (lr~0.00992)\n",
            "[LS] step 0100/196 | loss 1.5088 | acc 41.32% | lr~0.00705 | evals/step~3\n",
            "[Eval] epoch 112/200 | Tr Loss 1.5124 | Tr Acc 41.53% | Val Loss 2.0281 | Val Acc 33.02% (lr~0.00877)\n",
            "[LS] step 0100/196 | loss 1.5237 | acc 41.40% | lr~0.00824 | evals/step~3\n",
            "[Eval] epoch 113/200 | Tr Loss 1.5090 | Tr Acc 41.81% | Val Loss 2.0170 | Val Acc 33.42% (lr~0.00857)\n",
            "[LS] step 0100/196 | loss 1.5039 | acc 41.68% | lr~0.00593 | evals/step~3\n",
            "[Eval] epoch 114/200 | Tr Loss 1.5023 | Tr Acc 41.79% | Val Loss 2.0515 | Val Acc 31.64% (lr~0.00738)\n",
            "[LS] step 0100/196 | loss 1.4733 | acc 42.69% | lr~0.00395 | evals/step~3\n",
            "[Eval] epoch 115/200 | Tr Loss 1.4745 | Tr Acc 42.53% | Val Loss 2.0855 | Val Acc 34.00% (lr~0.00731)\n",
            "[LS] step 0100/196 | loss 1.4824 | acc 42.20% | lr~0.0109 | evals/step~2\n",
            "[Eval] epoch 116/200 | Tr Loss 1.4863 | Tr Acc 42.20% | Val Loss 2.1265 | Val Acc 32.69% (lr~0.0117)\n",
            "[LS] step 0100/196 | loss 1.4691 | acc 42.47% | lr~0.00913 | evals/step~4\n",
            "[Eval] epoch 117/200 | Tr Loss 1.4754 | Tr Acc 42.51% | Val Loss 2.0299 | Val Acc 34.50% (lr~0.00851)\n",
            "[LS] step 0100/196 | loss 1.4880 | acc 42.43% | lr~0.0104 | evals/step~3\n",
            "[Eval] epoch 118/200 | Tr Loss 1.4920 | Tr Acc 42.29% | Val Loss 2.1329 | Val Acc 33.82% (lr~0.0118)\n",
            "[LS] step 0100/196 | loss 1.4563 | acc 42.78% | lr~0.00639 | evals/step~7\n",
            "[Eval] epoch 119/200 | Tr Loss 1.4604 | Tr Acc 42.78% | Val Loss 2.1262 | Val Acc 32.17% (lr~0.00635)\n",
            "[LS] step 0100/196 | loss 1.4700 | acc 42.39% | lr~0.0087 | evals/step~2\n",
            "[Eval] epoch 120/200 | Tr Loss 1.4930 | Tr Acc 41.91% | Val Loss 2.0842 | Val Acc 33.68% (lr~0.00819)\n",
            "[LS] step 0100/196 | loss 1.5011 | acc 42.05% | lr~0.00788 | evals/step~3\n",
            "[Eval] epoch 121/200 | Tr Loss 1.4937 | Tr Acc 42.20% | Val Loss 2.1809 | Val Acc 33.67% (lr~0.0084)\n",
            "[LS] step 0100/196 | loss 1.4740 | acc 42.99% | lr~0.0113 | evals/step~2\n",
            "[Eval] epoch 122/200 | Tr Loss 1.4844 | Tr Acc 42.71% | Val Loss 2.0027 | Val Acc 31.97% (lr~0.0119)\n",
            "[LS] step 0100/196 | loss 1.5319 | acc 39.52% | lr~0.00863 | evals/step~3\n",
            "[Eval] epoch 123/200 | Tr Loss 1.5334 | Tr Acc 40.20% | Val Loss 2.0548 | Val Acc 33.42% (lr~0.011)\n",
            "[LS] step 0100/196 | loss 1.5505 | acc 40.30% | lr~0.0142 | evals/step~3\n",
            "[Eval] epoch 124/200 | Tr Loss 1.5429 | Tr Acc 40.36% | Val Loss 2.2239 | Val Acc 33.42% (lr~0.0138)\n",
            "[LS] step 0100/196 | loss 1.5361 | acc 41.05% | lr~0.0126 | evals/step~4\n",
            "[Eval] epoch 125/200 | Tr Loss 1.5323 | Tr Acc 40.99% | Val Loss 1.9094 | Val Acc 33.94% (lr~0.0116)\n",
            "[LS] step 0100/196 | loss 1.4915 | acc 42.36% | lr~0.00713 | evals/step~3\n",
            "[Eval] epoch 126/200 | Tr Loss 1.5028 | Tr Acc 42.16% | Val Loss 1.9761 | Val Acc 33.12% (lr~0.0112)\n",
            "[LS] step 0100/196 | loss 1.5079 | acc 42.59% | lr~0.0131 | evals/step~2\n",
            "[Eval] epoch 127/200 | Tr Loss 1.5135 | Tr Acc 42.09% | Val Loss 2.0292 | Val Acc 36.07% (lr~0.0136)\n",
            "New best model saved with validation accuracy: 36.07%\n",
            "[LS] step 0100/196 | loss 1.5018 | acc 41.89% | lr~0.0137 | evals/step~2\n",
            "[Eval] epoch 128/200 | Tr Loss 1.5161 | Tr Acc 41.53% | Val Loss 2.2255 | Val Acc 26.51% (lr~0.015)\n",
            "[LS] step 0100/196 | loss 1.5500 | acc 40.26% | lr~0.0102 | evals/step~2\n",
            "[Eval] epoch 129/200 | Tr Loss 1.5393 | Tr Acc 40.65% | Val Loss 2.4923 | Val Acc 33.02% (lr~0.0139)\n",
            "[LS] step 0100/196 | loss 1.5103 | acc 41.73% | lr~0.0154 | evals/step~3\n",
            "[Eval] epoch 130/200 | Tr Loss 1.5029 | Tr Acc 41.86% | Val Loss 2.1487 | Val Acc 33.28% (lr~0.0124)\n",
            "[LS] step 0100/196 | loss 1.5129 | acc 40.95% | lr~0.0132 | evals/step~2\n",
            "[Eval] epoch 131/200 | Tr Loss 1.5205 | Tr Acc 41.19% | Val Loss 1.9905 | Val Acc 32.28% (lr~0.0135)\n",
            "[LS] step 0100/196 | loss 1.5423 | acc 40.67% | lr~0.0141 | evals/step~3\n",
            "[Eval] epoch 132/200 | Tr Loss 1.5352 | Tr Acc 40.70% | Val Loss 2.2066 | Val Acc 32.07% (lr~0.0141)\n",
            "[LS] step 0100/196 | loss 1.5025 | acc 41.64% | lr~0.013 | evals/step~7\n",
            "[Eval] epoch 133/200 | Tr Loss 1.5121 | Tr Acc 41.68% | Val Loss 2.3597 | Val Acc 30.96% (lr~0.0143)\n",
            "[LS] step 0100/196 | loss 1.5717 | acc 40.21% | lr~0.00959 | evals/step~7\n",
            "[Eval] epoch 134/200 | Tr Loss 1.5394 | Tr Acc 40.91% | Val Loss 2.0967 | Val Acc 34.00% (lr~0.00812)\n",
            "[LS] step 0100/196 | loss 1.4908 | acc 41.95% | lr~0.0126 | evals/step~6\n",
            "[Eval] epoch 135/200 | Tr Loss 1.5055 | Tr Acc 41.50% | Val Loss 1.9612 | Val Acc 34.28% (lr~0.0114)\n",
            "[LS] step 0100/196 | loss 1.5261 | acc 41.30% | lr~0.0194 | evals/step~3\n",
            "[Eval] epoch 136/200 | Tr Loss 1.5223 | Tr Acc 41.27% | Val Loss 2.0979 | Val Acc 33.94% (lr~0.0154)\n",
            "[LS] step 0100/196 | loss 1.5165 | acc 40.99% | lr~0.0108 | evals/step~3\n",
            "[Eval] epoch 137/200 | Tr Loss 1.5185 | Tr Acc 41.17% | Val Loss 2.0811 | Val Acc 32.78% (lr~0.013)\n",
            "[LS] step 0100/196 | loss 1.5754 | acc 39.64% | lr~0.00886 | evals/step~2\n",
            "[Eval] epoch 138/200 | Tr Loss 1.5899 | Tr Acc 39.64% | Val Loss 2.1924 | Val Acc 26.27% (lr~0.00817)\n",
            "[LS] step 0100/196 | loss 1.5249 | acc 40.97% | lr~0.00727 | evals/step~3\n",
            "[Eval] epoch 139/200 | Tr Loss 1.5314 | Tr Acc 41.34% | Val Loss 2.1627 | Val Acc 32.31% (lr~0.0115)\n",
            "[LS] step 0100/196 | loss 1.5379 | acc 40.98% | lr~0.0109 | evals/step~7\n",
            "[Eval] epoch 140/200 | Tr Loss 1.5515 | Tr Acc 40.93% | Val Loss 1.9317 | Val Acc 33.10% (lr~0.0103)\n",
            "[LS] step 0100/196 | loss 1.5155 | acc 41.98% | lr~0.00985 | evals/step~3\n",
            "[Eval] epoch 141/200 | Tr Loss 1.5203 | Tr Acc 41.95% | Val Loss 1.9845 | Val Acc 34.65% (lr~0.0139)\n",
            "[LS] step 0100/196 | loss 1.4892 | acc 42.74% | lr~0.0137 | evals/step~3\n",
            "[Eval] epoch 142/200 | Tr Loss 1.5112 | Tr Acc 42.22% | Val Loss 2.1233 | Val Acc 32.70% (lr~0.0135)\n",
            "[LS] step 0100/196 | loss 1.4978 | acc 42.47% | lr~0.0149 | evals/step~2\n",
            "[Eval] epoch 143/200 | Tr Loss 1.5022 | Tr Acc 42.14% | Val Loss 1.9786 | Val Acc 34.21% (lr~0.00928)\n",
            "[LS] step 0100/196 | loss 1.5325 | acc 41.73% | lr~0.0178 | evals/step~3\n",
            "[Eval] epoch 144/200 | Tr Loss 1.5409 | Tr Acc 41.62% | Val Loss 2.1167 | Val Acc 33.07% (lr~0.0216)\n",
            "[LS] step 0100/196 | loss 1.5387 | acc 41.48% | lr~0.017 | evals/step~2\n",
            "[Eval] epoch 145/200 | Tr Loss 1.5288 | Tr Acc 41.59% | Val Loss 2.0079 | Val Acc 33.99% (lr~0.0141)\n",
            "[LS] step 0100/196 | loss 1.4926 | acc 42.30% | lr~0.0118 | evals/step~2\n",
            "[Eval] epoch 146/200 | Tr Loss 1.4780 | Tr Acc 42.80% | Val Loss 2.0943 | Val Acc 34.84% (lr~0.0101)\n",
            "[LS] step 0100/196 | loss 1.4605 | acc 43.76% | lr~0.0132 | evals/step~2\n",
            "[Eval] epoch 147/200 | Tr Loss 1.4555 | Tr Acc 43.78% | Val Loss 2.1004 | Val Acc 34.65% (lr~0.0124)\n",
            "[LS] step 0100/196 | loss 1.4611 | acc 43.48% | lr~0.0118 | evals/step~3\n",
            "[Eval] epoch 148/200 | Tr Loss 1.4512 | Tr Acc 43.86% | Val Loss 2.1320 | Val Acc 34.53% (lr~0.0133)\n",
            "[LS] step 0100/196 | loss 1.4464 | acc 44.59% | lr~0.0182 | evals/step~3\n",
            "[Eval] epoch 149/200 | Tr Loss 1.4450 | Tr Acc 44.52% | Val Loss 2.0251 | Val Acc 35.17% (lr~0.0175)\n",
            "[LS] step 0100/196 | loss 1.4399 | acc 44.68% | lr~0.0203 | evals/step~6\n",
            "[Eval] epoch 150/200 | Tr Loss 1.4489 | Tr Acc 44.22% | Val Loss 2.0917 | Val Acc 35.24% (lr~0.0142)\n",
            "[LS] step 0100/196 | loss 1.4493 | acc 44.41% | lr~0.0141 | evals/step~3\n",
            "[Eval] epoch 151/200 | Tr Loss 1.4504 | Tr Acc 44.22% | Val Loss 2.1375 | Val Acc 35.60% (lr~0.0133)\n",
            "[LS] step 0100/196 | loss 1.4373 | acc 44.83% | lr~0.00902 | evals/step~3\n",
            "[Eval] epoch 152/200 | Tr Loss 1.4412 | Tr Acc 44.66% | Val Loss 2.0950 | Val Acc 35.77% (lr~0.0112)\n",
            "[LS] step 0100/196 | loss 1.4420 | acc 44.46% | lr~0.0193 | evals/step~2\n",
            "[Eval] epoch 153/200 | Tr Loss 1.4704 | Tr Acc 43.55% | Val Loss 2.0895 | Val Acc 33.71% (lr~0.0167)\n",
            "[LS] step 0100/196 | loss 1.4769 | acc 43.28% | lr~0.0176 | evals/step~3\n",
            "[Eval] epoch 154/200 | Tr Loss 1.4631 | Tr Acc 43.63% | Val Loss 2.1062 | Val Acc 33.97% (lr~0.0169)\n",
            "[LS] step 0100/196 | loss 1.4568 | acc 43.73% | lr~0.0172 | evals/step~2\n",
            "[Eval] epoch 155/200 | Tr Loss 1.4498 | Tr Acc 43.22% | Val Loss 2.0335 | Val Acc 34.46% (lr~0.0115)\n",
            "[LS] step 0100/196 | loss 1.4300 | acc 43.90% | lr~0.00454 | evals/step~3\n",
            "[Eval] epoch 156/200 | Tr Loss 1.4363 | Tr Acc 44.07% | Val Loss 2.0763 | Val Acc 34.13% (lr~0.00842)\n",
            "[LS] step 0100/196 | loss 1.4327 | acc 44.49% | lr~0.00826 | evals/step~3\n",
            "[Eval] epoch 157/200 | Tr Loss 1.4356 | Tr Acc 44.19% | Val Loss 2.1172 | Val Acc 34.18% (lr~0.00676)\n",
            "[LS] step 0100/196 | loss 1.4193 | acc 44.43% | lr~0.00663 | evals/step~3\n",
            "[Eval] epoch 158/200 | Tr Loss 1.4203 | Tr Acc 44.45% | Val Loss 2.1877 | Val Acc 33.80% (lr~0.00715)\n",
            "[LS] step 0100/196 | loss 1.4360 | acc 43.70% | lr~0.013 | evals/step~2\n",
            "[Eval] epoch 159/200 | Tr Loss 1.4560 | Tr Acc 43.35% | Val Loss 2.1951 | Val Acc 36.31% (lr~0.0121)\n",
            "New best model saved with validation accuracy: 36.31%\n",
            "[LS] step 0100/196 | loss 1.4529 | acc 43.98% | lr~0.0169 | evals/step~3\n",
            "[Eval] epoch 160/200 | Tr Loss 1.4529 | Tr Acc 43.86% | Val Loss 2.2113 | Val Acc 35.27% (lr~0.0157)\n",
            "[LS] step 0100/196 | loss 1.4535 | acc 43.65% | lr~0.0197 | evals/step~3\n",
            "[Eval] epoch 161/200 | Tr Loss 1.4535 | Tr Acc 44.04% | Val Loss 2.0653 | Val Acc 35.46% (lr~0.0154)\n",
            "[LS] step 0100/196 | loss 1.4569 | acc 44.64% | lr~0.0176 | evals/step~3\n",
            "[Eval] epoch 162/200 | Tr Loss 1.4671 | Tr Acc 43.85% | Val Loss 2.2278 | Val Acc 34.17% (lr~0.0176)\n",
            "[LS] step 0100/196 | loss 1.4354 | acc 45.11% | lr~0.0139 | evals/step~3\n",
            "[Eval] epoch 163/200 | Tr Loss 1.4453 | Tr Acc 44.70% | Val Loss 2.5761 | Val Acc 34.23% (lr~0.0123)\n",
            "[LS] step 0100/196 | loss 1.4496 | acc 44.99% | lr~0.0212 | evals/step~2\n",
            "[Eval] epoch 164/200 | Tr Loss 1.4435 | Tr Acc 44.97% | Val Loss 2.3751 | Val Acc 35.38% (lr~0.0212)\n",
            "[LS] step 0100/196 | loss 1.4738 | acc 43.54% | lr~0.0145 | evals/step~2\n",
            "[Eval] epoch 165/200 | Tr Loss 1.4624 | Tr Acc 43.82% | Val Loss 2.4582 | Val Acc 35.11% (lr~0.0164)\n",
            "[LS] step 0100/196 | loss 1.4338 | acc 44.50% | lr~0.0179 | evals/step~2\n",
            "[Eval] epoch 166/200 | Tr Loss 1.4278 | Tr Acc 44.57% | Val Loss 2.1770 | Val Acc 35.13% (lr~0.0143)\n",
            "[LS] step 0100/196 | loss 1.4138 | acc 44.25% | lr~0.00881 | evals/step~2\n",
            "[Eval] epoch 167/200 | Tr Loss 1.4173 | Tr Acc 44.31% | Val Loss 2.6921 | Val Acc 33.53% (lr~0.00866)\n",
            "[LS] step 0100/196 | loss 1.4128 | acc 44.61% | lr~0.0119 | evals/step~3\n",
            "[Eval] epoch 168/200 | Tr Loss 1.4185 | Tr Acc 44.63% | Val Loss 2.3699 | Val Acc 35.47% (lr~0.0145)\n",
            "[LS] step 0100/196 | loss 1.4109 | acc 44.73% | lr~0.0127 | evals/step~3\n",
            "[Eval] epoch 169/200 | Tr Loss 1.4181 | Tr Acc 44.48% | Val Loss 2.2792 | Val Acc 34.59% (lr~0.0135)\n",
            "[LS] step 0100/196 | loss 1.3942 | acc 45.66% | lr~0.00987 | evals/step~3\n",
            "[Eval] epoch 170/200 | Tr Loss 1.4040 | Tr Acc 45.49% | Val Loss 2.2705 | Val Acc 35.19% (lr~0.0116)\n",
            "[LS] step 0100/196 | loss 1.4323 | acc 45.00% | lr~0.0139 | evals/step~3\n",
            "[Eval] epoch 171/200 | Tr Loss 1.4457 | Tr Acc 44.28% | Val Loss 2.2029 | Val Acc 34.70% (lr~0.0115)\n",
            "[LS] step 0100/196 | loss 1.4384 | acc 43.90% | lr~0.012 | evals/step~2\n",
            "[Eval] epoch 172/200 | Tr Loss 1.4385 | Tr Acc 44.17% | Val Loss 2.1480 | Val Acc 35.19% (lr~0.0143)\n",
            "[LS] step 0100/196 | loss 1.4014 | acc 44.77% | lr~0.00868 | evals/step~3\n",
            "[Eval] epoch 173/200 | Tr Loss 1.4308 | Tr Acc 44.72% | Val Loss 2.2122 | Val Acc 35.20% (lr~0.0132)\n",
            "[LS] step 0100/196 | loss 1.4841 | acc 43.55% | lr~0.0134 | evals/step~4\n",
            "[Eval] epoch 174/200 | Tr Loss 1.4677 | Tr Acc 43.98% | Val Loss 2.2266 | Val Acc 35.57% (lr~0.0135)\n",
            "[LS] step 0100/196 | loss 1.4529 | acc 44.00% | lr~0.0139 | evals/step~3\n",
            "[Eval] epoch 175/200 | Tr Loss 1.4669 | Tr Acc 43.69% | Val Loss 1.9896 | Val Acc 33.50% (lr~0.015)\n",
            "[LS] step 0100/196 | loss 1.4512 | acc 44.00% | lr~0.0183 | evals/step~3\n",
            "[Eval] epoch 176/200 | Tr Loss 1.4636 | Tr Acc 43.53% | Val Loss 2.1561 | Val Acc 34.42% (lr~0.0178)\n",
            "[LS] step 0100/196 | loss 1.4695 | acc 42.77% | lr~0.0216 | evals/step~3\n",
            "[Eval] epoch 177/200 | Tr Loss 1.4578 | Tr Acc 43.10% | Val Loss 2.1220 | Val Acc 35.45% (lr~0.017)\n",
            "[LS] step 0100/196 | loss 1.4369 | acc 43.88% | lr~0.0132 | evals/step~2\n",
            "[Eval] epoch 178/200 | Tr Loss 1.4418 | Tr Acc 43.92% | Val Loss 2.1808 | Val Acc 34.53% (lr~0.0173)\n",
            "[LS] step 0100/196 | loss 1.4931 | acc 43.33% | lr~0.0168 | evals/step~3\n",
            "[Eval] epoch 179/200 | Tr Loss 1.4826 | Tr Acc 43.15% | Val Loss 2.0954 | Val Acc 33.48% (lr~0.0121)\n",
            "[LS] step 0100/196 | loss 1.4836 | acc 42.98% | lr~0.0147 | evals/step~3\n",
            "[Eval] epoch 180/200 | Tr Loss 1.4665 | Tr Acc 44.20% | Val Loss 2.1074 | Val Acc 35.97% (lr~0.0152)\n",
            "[LS] step 0100/196 | loss 1.4248 | acc 45.26% | lr~0.011 | evals/step~7\n",
            "[Eval] epoch 181/200 | Tr Loss 1.4251 | Tr Acc 45.32% | Val Loss 2.2549 | Val Acc 35.76% (lr~0.00993)\n",
            "[LS] step 0100/196 | loss 1.4378 | acc 45.12% | lr~0.0117 | evals/step~2\n",
            "[Eval] epoch 182/200 | Tr Loss 1.4443 | Tr Acc 44.64% | Val Loss 2.2483 | Val Acc 33.76% (lr~0.0145)\n",
            "[LS] step 0100/196 | loss 1.4432 | acc 44.64% | lr~0.0204 | evals/step~2\n",
            "[Eval] epoch 183/200 | Tr Loss 1.4607 | Tr Acc 44.25% | Val Loss 2.2645 | Val Acc 36.28% (lr~0.0202)\n",
            "[LS] step 0100/196 | loss 1.4253 | acc 45.93% | lr~0.0123 | evals/step~3\n",
            "[Eval] epoch 184/200 | Tr Loss 1.4249 | Tr Acc 46.17% | Val Loss 2.1803 | Val Acc 36.94% (lr~0.0154)\n",
            "New best model saved with validation accuracy: 36.94%\n",
            "[LS] step 0100/196 | loss 1.3842 | acc 47.17% | lr~0.00846 | evals/step~2\n",
            "[Eval] epoch 185/200 | Tr Loss 1.3898 | Tr Acc 47.07% | Val Loss 2.4462 | Val Acc 36.38% (lr~0.0114)\n",
            "[LS] step 0100/196 | loss 1.4088 | acc 46.72% | lr~0.0152 | evals/step~2\n",
            "[Eval] epoch 186/200 | Tr Loss 1.4156 | Tr Acc 46.62% | Val Loss 2.3321 | Val Acc 34.46% (lr~0.0132)\n",
            "[LS] step 0100/196 | loss 1.3787 | acc 47.68% | lr~0.0104 | evals/step~2\n",
            "[Eval] epoch 187/200 | Tr Loss 1.4266 | Tr Acc 46.57% | Val Loss 2.6205 | Val Acc 36.86% (lr~0.0132)\n",
            "[LS] step 0100/196 | loss 1.4205 | acc 46.77% | lr~0.0114 | evals/step~2\n",
            "[Eval] epoch 188/200 | Tr Loss 1.4310 | Tr Acc 46.48% | Val Loss 2.1717 | Val Acc 36.54% (lr~0.0141)\n",
            "[LS] step 0100/196 | loss 1.4342 | acc 46.48% | lr~0.0187 | evals/step~5\n",
            "[Eval] epoch 189/200 | Tr Loss 1.4327 | Tr Acc 46.63% | Val Loss 2.0259 | Val Acc 36.19% (lr~0.0156)\n",
            "[LS] step 0100/196 | loss 1.4040 | acc 46.62% | lr~0.0119 | evals/step~3\n",
            "[Eval] epoch 190/200 | Tr Loss 1.4063 | Tr Acc 46.74% | Val Loss 2.2302 | Val Acc 35.36% (lr~0.0163)\n",
            "[LS] step 0100/196 | loss 1.4483 | acc 45.41% | lr~0.0206 | evals/step~3\n",
            "[Eval] epoch 191/200 | Tr Loss 1.4372 | Tr Acc 45.50% | Val Loss 2.2490 | Val Acc 36.31% (lr~0.0179)\n",
            "[LS] step 0100/196 | loss 1.4424 | acc 45.79% | lr~0.0237 | evals/step~3\n",
            "[Eval] epoch 192/200 | Tr Loss 1.4430 | Tr Acc 45.25% | Val Loss 2.3395 | Val Acc 35.90% (lr~0.0201)\n",
            "[LS] step 0100/196 | loss 1.4163 | acc 45.73% | lr~0.0198 | evals/step~7\n",
            "[Eval] epoch 193/200 | Tr Loss 1.4312 | Tr Acc 45.29% | Val Loss 2.1829 | Val Acc 34.39% (lr~0.0165)\n",
            "[LS] step 0100/196 | loss 1.4321 | acc 44.86% | lr~0.0176 | evals/step~2\n",
            "[Eval] epoch 194/200 | Tr Loss 1.4275 | Tr Acc 44.80% | Val Loss 2.3424 | Val Acc 34.42% (lr~0.0151)\n",
            "[LS] step 0100/196 | loss 1.4194 | acc 44.84% | lr~0.0151 | evals/step~2\n",
            "[Eval] epoch 195/200 | Tr Loss 1.4200 | Tr Acc 44.76% | Val Loss 1.9567 | Val Acc 34.97% (lr~0.0118)\n",
            "[LS] step 0100/196 | loss 1.4153 | acc 45.44% | lr~0.021 | evals/step~3\n",
            "[Eval] epoch 196/200 | Tr Loss 1.4208 | Tr Acc 44.95% | Val Loss 2.0672 | Val Acc 34.50% (lr~0.0161)\n",
            "[LS] step 0100/196 | loss 1.3827 | acc 46.02% | lr~0.0136 | evals/step~3\n",
            "[Eval] epoch 197/200 | Tr Loss 1.3937 | Tr Acc 45.94% | Val Loss 2.0088 | Val Acc 35.03% (lr~0.0159)\n",
            "[LS] step 0100/196 | loss 1.4083 | acc 46.15% | lr~0.0158 | evals/step~2\n",
            "[Eval] epoch 198/200 | Tr Loss 1.4112 | Tr Acc 45.90% | Val Loss 2.2803 | Val Acc 34.35% (lr~0.0155)\n",
            "[LS] step 0100/196 | loss 1.4271 | acc 45.70% | lr~0.024 | evals/step~2\n",
            "[Eval] epoch 199/200 | Tr Loss 1.4255 | Tr Acc 45.12% | Val Loss 2.1110 | Val Acc 33.94% (lr~0.0197)\n",
            "[LS] step 0100/196 | loss 1.3838 | acc 45.82% | lr~0.0104 | evals/step~3\n",
            "[Eval] epoch 200/200 | Tr Loss 1.4048 | Tr Acc 45.47% | Val Loss 2.2523 | Val Acc 35.25% (lr~0.0126)\n",
            "\n",
            "Training complete.\n",
            "Best validation accuracy achieved: 36.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb65d54"
      },
      "source": [
        "## Summary of Results and Observations\n",
        "\n",
        "After training the 5-layer MLP model on the CIFAR-10 dataset for 200 epochs using the `LineSearchMuon` controller with the specified hyperparameters, here are the key results and observations:\n",
        "\n",
        "*   **Best Validation Accuracy:** The model achieved a best validation accuracy of **36.94%**.\n",
        "*   **Training Loss and Accuracy:** Over the 200 epochs, the training loss generally decreased, and training accuracy increased, indicating that the model was learning from the data. However, the improvements were gradual and somewhat noisy.\n",
        "*   **Validation Loss and Accuracy Trends:** The validation accuracy showed fluctuations throughout the training process. While it improved from an initial ~26% to the peak of 36.94%, there were periods where it dropped before recovering.\n",
        "*   **Line Search Learning Rate (LR) Behavior:** The line search controller dynamically adjusted the learning rate for each step. The average learning rate (`lr~`) printed in the logs showed variability, often oscillating around the initial `init_lr` of 2e-2. The `evals/step` metric indicates that the line search performed between 2 to 7 evaluations per step to determine the optimal learning rate, which is within the configured `max_grow` and `max_backtrack` limits.\n",
        "*   **Model Performance:** A 5-layer MLP on CIFAR-10 is a relatively simple architecture for this dataset, which typically benefits more from convolutional layers. The achieved accuracy of 36.94% is modest but expected for an MLP on CIFAR-10 without advanced techniques like convolutions, batch normalization, or more sophisticated regularization. The line search mechanism helped in adapting the learning rate, but the inherent complexity of the dataset for a simple MLP architecture limited the overall performance.\n",
        "*   **Stability of Training:** The training appeared relatively stable, with no clear signs of divergence, suggesting that the line search effectively prevented excessively large steps that could destabilize the training.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The `LineSearchMuon` optimizer successfully managed the learning rate adaptively throughout the training, showing its ability to find suitable step sizes. While the model showed learning, the overall performance highlights the need for more complex architectures (like CNNs) when dealing with image datasets such as CIFAR-10 to achieve higher accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69df055f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the results and observations from training the 5-layer MLP on CIFAR-10 using the `LineSearchMuon` for 200 epochs, highlighting any insights into the optimizer's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ca48f4"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The 5-layer Multi-Layer Perceptron (MLP) model, trained on the CIFAR-10 dataset for 200 epochs using the `LineSearchMuon` optimizer, achieved a **best validation accuracy of 36.94%**.\n",
        "*   The `LineSearchMuon` controller successfully managed adaptive learning rates throughout the training process. Logs showed the line search typically performed between 2 to 7 evaluations per step to determine an optimal learning rate, staying within the configured `max_grow` (3) and `max_backtrack` (6) limits.\n",
        "*   Training exhibited stability, with the `LineSearchMuon` preventing divergence by adaptively adjusting the learning rate. Training loss generally decreased and training accuracy increased, while validation accuracy showed overall improvement despite some fluctuations.\n",
        "*   The initial `init_lr` for the `LineSearchMuon` was set to 2e-2, with other key hyperparameters including `c=0.2`, `growth=1.4`, `shrink=0.5`, `lr_min=5e-6`, `lr_max=1.0`.\n",
        "*   The modest validation accuracy of 36.94% is expected given the use of a simple 5-layer MLP architecture for a complex image dataset like CIFAR-10, which typically benefits more from convolutional neural networks. The training was conducted on a CPU.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Insight:** The `LineSearchMuon` optimizer effectively provides adaptive learning rate management, ensuring stable training by adjusting step sizes based on an Armijo-like condition. However, the model's architectural choice (MLP) is a primary limiting factor for performance on the CIFAR-10 image dataset, regardless of the optimizer's efficiency.\n",
        "*   **Next Steps:** To significantly improve performance on CIFAR-10, future work should involve switching to a Convolutional Neural Network (CNN) architecture. Further experimentation with `LineSearchMuon` hyperparameters on a CNN model could then be conducted to optimize performance within that more suitable architecture.\n"
      ]
    }
  ]
}